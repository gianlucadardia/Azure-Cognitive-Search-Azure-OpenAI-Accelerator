{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a8b5c0-87cb-4302-8e3c-dc809d0039fb",
   "metadata": {},
   "source": [
    "# Understanding Memory in LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f73380-6395-4e9f-9c83-3f47a5d7e292",
   "metadata": {},
   "source": [
    "In the previous Notebook 03, we successfully explored how OpenAI models can enhance the results from Azure Cognitive Search. [Bing Chat](http://chat.bing.com/) is a search engine with a GPT-4 model that utilizes the content of search results to provide context and deliver accurate responses to queries.\n",
    "\n",
    "However, we have yet to discover how to engage in a conversation with the LLM. With Bing Chat, this is possible, as the LLM can understand and reference the previous responses.\n",
    "\n",
    "There is a common misconception that GPT models have memory. This is not true. While they possess knowledge, they do not retain information from previous questions asked to them.\n",
    "\n",
    "The aim of this Notebook is to demonstrate how we can \"provide memory\" to the LLM by utilizing prompts and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "733c782e-204c-47d0-8dae-c9df7091ab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT\n",
    "from langchain.memory import ConversationBufferMemory, ConversationTokenBufferMemory\n",
    "from openai.error import OpenAIError\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.memory import CosmosDBChatMessageHistory\n",
    "\n",
    "from IPython.display import Markdown, HTML, display  \n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "#custom libraries that we will use later in the app\n",
    "from common.utils import (\n",
    "    get_search_results,\n",
    "    order_search_results,\n",
    "    model_tokens_limit,\n",
    "    num_tokens_from_docs,\n",
    "    embed_docs,\n",
    "    search_docs,\n",
    "    get_answer,\n",
    ")\n",
    "\n",
    "from common.prompts import COMBINE_QUESTION_PROMPT, COMBINE_PROMPT, COMBINE_CHAT_PROMPT\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"credentials.env\")\n",
    "\n",
    "import logging\n",
    "\n",
    "# Get the root logger\n",
    "logger = logging.getLogger()\n",
    "# Set the logging level to a higher level to ignore INFO messages\n",
    "logger.setLevel(logging.WARNING)\n",
    "\n",
    "#DATASOURCE_SAS_TOKEN = \"?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2023-11-29T01:50:59Z&st=2023-05-10T16:50:59Z&spr=https&sig=ZT7MLy%2BnlvAxUKKj5v0RwoWObXaab3gO4ec2%2Fci6iL0%3D\"\n",
    "DATASOURCE_SAS_TOKEN = \"?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bc63c55-a57d-49a7-b6c7-0f18bca8199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
    "os.environ[\"OPENAI_API_BASE\"] = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "os.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc72b22-11c2-4df0-91b8-033d01829663",
   "metadata": {},
   "source": [
    "### Let's start with the basics\n",
    "Let's use a very simple example to see if the GPT model of Azure OpenAI have memory. We again will be using langchain to simplify our code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eef5dc9-8b80-4085-980c-865fa41fa1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Tell me some use cases for reinforcement learning?\"\n",
    "FOLLOW_UP_QUESTION = \"Can you summarize your last response?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a00181d5-bd76-4ce4-a256-75ac5b58c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "MODEL = \"gpt-35-turbo\"\n",
    "# Create an OpenAI instance\n",
    "llm = AzureChatOpenAI(deployment_name=MODEL, temperature=0.5, max_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9502d0f1-fddf-40d1-95d2-a1461dcc498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a very simple prompt template, just the question as is:\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"{question}\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5c9903e-15c7-4e05-87a1-58e5a7917ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. Robotics: Reinforcement learning can be used to train robots to perform complex tasks, such as grasping objects, navigating through environments, and even playing games.\n",
       "\n",
       "2. Gaming: Reinforcement learning has been used to create AI agents that can play games like chess, Go, and poker at a professional level.\n",
       "\n",
       "3. Advertising: Reinforcement learning can be used to optimize advertising campaigns by learning which ads perform best for different audiences.\n",
       "\n",
       "4. Finance: Reinforcement learning can be used to optimize investment strategies by learning from past market data and making predictions about future trends.\n",
       "\n",
       "5. Healthcare: Reinforcement learning can be used to optimize treatment plans for patients by learning from patient data and adjusting treatment plans based on patient outcomes.\n",
       "\n",
       "6. Transportation: Reinforcement learning can be used to optimize traffic flow and reduce congestion by learning from traffic data and adjusting traffic signals in real-time.\n",
       "\n",
       "7. Energy: Reinforcement learning can be used to optimize energy consumption by learning from energy usage patterns and adjusting energy production accordingly.\n",
       "\n",
       "8. Manufacturing: Reinforcement learning can be used to optimize production processes by learning from production data and adjusting production schedules and processes to improve efficiency."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's see what the GPT model responds\n",
    "response = chain.run(QUESTION)\n",
    "printmd(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99acaf3c-ce68-4b87-b24a-6065b15ff9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As an AI language model, I do not have the ability to recall my last response. However, I can provide a summary of my current response or any other response if you let me know which one you are referring to.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now let's ask a follow up question\n",
    "chain.run(FOLLOW_UP_QUESTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e1c143-c95f-4566-a8b4-af8c42f08dd2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "As you can see, it doesn't remember what it just responded. This proof that the LLM does NOT have memory and that we need to give the memory as a a conversation history as part of the prompt, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0946ce71-6285-432e-b011-9c2dc1ba7b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"question\"],\n",
    "    template=\"\"\"\n",
    "                {history}\n",
    "                Human: {question}\n",
    "                AI:\n",
    "            \"\"\"\n",
    "    )\n",
    "chain = LLMChain(llm=llm, prompt=hist_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d088e51-e5eb-4143-b87d-b2be429eb864",
   "metadata": {},
   "outputs": [],
   "source": [
    "Conversation_history = \"\"\"\n",
    "Human: {question}\n",
    "AI: {response}\n",
    "\"\"\".format(question=QUESTION, response=response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d99e34ad-5539-44dd-b080-3ad05efd2f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reinforcement learning has various use cases, including robotics, gaming, advertising, finance, healthcare, transportation, energy, and manufacturing. It can be used to optimize processes and make predictions based on data.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run({\"history\":Conversation_history, \"question\": FOLLOW_UP_QUESTION})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045e5af6-55d6-4353-b3f6-3275c95db00a",
   "metadata": {},
   "source": [
    "**Bingo!**, so we now know how to create a chatbot using LLMs, we just need to keep the state/history of the conversation and pass it as context every time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafd1694-0077-4aa8-bd01-e9f763ce36a3",
   "metadata": {},
   "source": [
    "## Now that we understand the concept of memory via adding history as a context, let's go back to our GPT Smart Search engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b27c45-7fbb-40da-a2e3-61e66a8e49b0",
   "metadata": {},
   "source": [
    "In order to not duplicate code, we have put many of the code used in Notebook 3 into functions. These functions are in the `common/utils.py` and `common/prompts.py` files This way we can use these functios in the app that we will build later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef9f459b-e8b8-40b9-a94d-80c079968594",
   "metadata": {},
   "outputs": [],
   "source": [
    "index1_name = \"cogsrch-index-files\"\n",
    "index2_name = \"cogsrch-index-csv\"\n",
    "indexes = [index1_name, index2_name]\n",
    "\n",
    "agg_search_results = get_search_results(QUESTION, indexes)\n",
    "ordered_results = order_search_results(agg_search_results, reranker_threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b2a3595-c3b7-4376-b9c5-0db7a42b3ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom token limit for gpt-35-turbo : 2500\n",
      "Combined docs tokens count: 59028\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "for key,value in ordered_results.items():\n",
    "    for page in value[\"chunks\"]:\n",
    "        location = value[\"location\"] if value[\"location\"] is not None else \"\"\n",
    "        docs.append(Document(page_content=page, metadata={\"source\": location+DATASOURCE_SAS_TOKEN}))\n",
    "\n",
    "# Calculate number of tokens of our docs\n",
    "tokens_limit = model_tokens_limit(MODEL)\n",
    "\n",
    "if(len(docs)>0):\n",
    "    num_tokens = num_tokens_from_docs(docs)\n",
    "    print(\"Custom token limit for\", MODEL, \":\", tokens_limit)\n",
    "    print(\"Combined docs tokens count:\",num_tokens)\n",
    "        \n",
    "else:\n",
    "    print(\"NO RESULTS FROM AZURE SEARCH\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c26d7540-feb8-4581-849e-003f4bf2a601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count after similarity search: 2043\n",
      "Chain Type selected: stuff\n",
      "CPU times: total: 484 ms\n",
      "Wall time: 3.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if num_tokens > tokens_limit:\n",
    "    index = embed_docs(docs)\n",
    "    top_docs = search_docs(index,QUESTION,k=2)\n",
    "    \n",
    "    # Now we need to recalculate the tokens count of the top results from similarity vector search\n",
    "    # in order to select the chain type: stuff or map_reduce\n",
    "    \n",
    "    num_tokens = num_tokens_from_docs(top_docs)   \n",
    "    print(\"Token count after similarity search:\", num_tokens)\n",
    "    chain_type = \"map_reduce\" if num_tokens > tokens_limit else \"stuff\"\n",
    "    \n",
    "else:\n",
    "    # if total tokens is less than our limit, we don't need to vectorize and do similarity search\n",
    "    top_docs = docs\n",
    "    chain_type = \"stuff\"\n",
    "    \n",
    "print(\"Chain Type selected:\", chain_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ce6efa9-2b8f-4810-904d-5986b4ae0372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Reinforcement learning can be applied in multi-robot domains to give rise to social behaviors and to individually program robots to produce certain group behaviors. It can also be used in market-based MASs to show how certain initial learning biases can be self-fulfilling and how learning can be useful but is affected by an agent's models of other agents. Other use cases include model-based learning, where agents build models of other agents via observations, and experimental studies of the behavior of reinforcement learning agents. However, the dynamics of MASs can be difficult to predict, especially when removing limitations from the CLRI framework. Therefore, reinforcement learning is best suited for simpler artificial programmable agents, rather than the complex behavior of humans or the unpredictable behavior of animals. <sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v2.pdf?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[25]</a></sup><sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v3.pdf?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[33]</a></sup>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the answer\n",
    "response = get_answer(llm=llm, docs=top_docs, query=QUESTION, language=\"English\", chain_type=chain_type)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27501f1b-7db0-4ee3-9cb1-e609254ffa3d",
   "metadata": {},
   "source": [
    "And if we ask the follow up question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cf5b323-3b9c-479b-8502-acfc4f7915dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I apologize, but I cannot summarize my last response as there is no previous response provided in this question."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = get_answer(llm=llm, docs=top_docs,  query=FOLLOW_UP_QUESTION, language=\"English\", chain_type=chain_type)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035fa6e6-226c-400f-a504-30255385f43b",
   "metadata": {},
   "source": [
    "You might get a different response from above, but it doesn't matter what response you get, it will be based on the context given, not on previous answers.\n",
    "\n",
    "Until now we just have the same as the prior Notebook 03: results from Azure Search enhanced by OpenAI model, with no memory\n",
    "\n",
    "**Now let's add memory to it:**\n",
    "\n",
    "Reference: https://python.langchain.com/docs/modules/memory/how_to/adding_memory_chain_multiple_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d98b876e-d264-48ae-b5ed-9801d6a9152b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Reinforcement learning has various use cases, including predictive strategies, social behaviors in multi-robot domains, and model-based learning. Some researchers have studied reinforcement learning in market-based MASs, showing how certain initial learning biases can be self-fulfilling, and how learning can be useful but affected by an agent's models of other agents. However, it is important to note that the behavior of multi-agent systems becomes much harder to predict as we remove limitations from the CLRI framework, which places constraints on the type of systems it can model. Therefore, the dynamics of these systems continue to be studied by complexity researchers with only modest progress. <sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v2.pdf?sv=2022-11-02&amp;ss=bf&amp;srt=co&amp;sp=rwdlaciytfx&amp;se=2024-07-31T19:43:39Z&amp;st=2023-07-31T11:43:39Z&amp;spr=https&amp;sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[25]</a></sup><sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v3.pdf?sv=2022-11-02&amp;ss=bf&amp;srt=co&amp;sp=rwdlaciytfx&amp;se=2024-07-31T19:43:39Z&amp;st=2023-07-31T11:43:39Z&amp;spr=https&amp;sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[11]</a></sup>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# memory object, which is neccessary to track the inputs/outputs and hold a conversation.\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",input_key=\"question\")\n",
    "\n",
    "response = get_answer(llm=llm, docs=top_docs, query=QUESTION, language=\"English\", chain_type=chain_type, \n",
    "                        memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf28927b-d9ee-4412-bb07-13e055e832a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In my previous response, I mentioned that reinforcement learning has various use cases, including predictive strategies, social behaviors in multi-robot domains, and model-based learning. Some researchers have studied reinforcement learning in market-based MASs, showing how certain initial learning biases can be self-fulfilling, and how learning can be useful but affected by an agent's models of other agents. However, it is important to note that the behavior of multi-agent systems becomes much harder to predict as we remove limitations from the CLRI framework, which places constraints on the type of systems it can model. Therefore, the dynamics of these systems continue to be studied by complexity researchers with only modest progress <sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v2.pdf?sv=2022-11-02&amp;ss=bf&amp;srt=co&amp;sp=rwdlaciytfx&amp;se=2024-07-31T19:43:39Z&amp;st=2023-07-31T11:43:39Z&amp;spr=https&amp;sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[25]</a></sup><sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v3.pdf?sv=2022-11-02&amp;ss=bf&amp;srt=co&amp;sp=rwdlaciytfx&amp;se=2024-07-31T19:43:39Z&amp;st=2023-07-31T11:43:39Z&amp;spr=https&amp;sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[11]</a></sup>."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we add a follow up question:\n",
    "response = get_answer(llm=llm, docs=top_docs, query=FOLLOW_UP_QUESTION, language=\"English\", chain_type=chain_type, \n",
    "                      memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3830b0b8-0ca2-4d0a-9747-f6273368002b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In my previous response, I mentioned that reinforcement learning has various use cases, including predictive strategies, social behaviors in multi-robot domains, and model-based learning. Some researchers have studied reinforcement learning in market-based MASs, showing how certain initial learning biases can be self-fulfilling, and how learning can be useful but affected by an agent's models of other agents. To elaborate, Matarić [22] has studied reinforcement learning in multi-robot domains, noting how learning can give rise to social behaviors [23]. Carmel and Markovitch [4] work on model-based learning, where agents build models of other agents via observations. They use models based on finite state machines. The authors show how some of these models can be effectively learned via observation of the other agent’s actions. There is also experimental work that has been done in the area of agents learning about agents [27, 36]. For example, Sen and Sekaran [28] show how learning agents in simple MAS converge to system-wide optimal behavior. Their agents use Q-learning or modified classifier systems in order to learn. Other researchers have extended the basic Q-learning [35] algorithm for use with MASs in an effort to either improve or prove convergence to the optimal behavior [32, 20, 13]. \n",
       "\n",
       "Sources: <sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v2.pdf?sv=2022-11-02&amp;ss=bf&amp;srt=co&amp;sp=rwdlaciytfx&amp;se=2024-07-31T19:43:39Z&amp;st=2023-07-31T11:43:39Z&amp;spr=https&amp;sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[25]</a></sup><sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v3.pdf?sv=2022-11-02&amp;ss=bf&amp;srt=co&amp;sp=rwdlaciytfx&amp;se=2024-07-31T19:43:39Z&amp;st=2023-07-31T11:43:39Z&amp;spr=https&amp;sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[11]</a></sup><sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v2.pdf?sv=2022-11-02&amp;ss=bf&amp;srt=co&amp;sp=rwdlaciytfx&amp;se=2024-07-31T19:43:39Z&amp;st=2023-07-31T11:43:39Z&amp;spr=https&amp;sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[22]</a></sup><sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v3.pdf?sv=2022-11-02&amp;ss=bf&amp;srt=co&amp;sp=rwdlaciytfx&amp;se=2024-07-31T19:43:39Z&amp;st=2023-07-31T11:43:39Z&amp;spr=https&amp;sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[4]</a></sup><sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v3.pdf?sv=2022-11-02&amp;ss=bf&amp;srt=co&amp;sp=rwdlaciytfx&amp;se=2024-07-31T19:43:39Z&amp;st=2023-07-31T11:43:39Z&amp;spr=https&amp;sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[27, 36]</a></sup><sup"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Another follow up query\n",
    "response = get_answer(llm=llm, docs=top_docs, query=\"Thank you\", language=\"English\", chain_type=chain_type,  \n",
    "                      memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111e732b-3c8c-4df3-8fcb-c3d01e7bec74",
   "metadata": {},
   "source": [
    "You might get a different answer on the above cell, and it is ok, this bot is not yet well configured to answer any question that is not related to its knowledge base, including salutations.\n",
    "\n",
    "Let's check our memory to see that it's keeping the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1279692c-7eb0-4300-8a66-c7025f02c318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Tell me some use cases for reinforcement learning?\\nAI: Reinforcement learning has various use cases, including predictive strategies, social behaviors in multi-robot domains, and model-based learning. Some researchers have studied reinforcement learning in market-based MASs, showing how certain initial learning biases can be self-fulfilling, and how learning can be useful but affected by an agent\\'s models of other agents. However, it is important to note that the behavior of multi-agent systems becomes much harder to predict as we remove limitations from the CLRI framework, which places constraints on the type of systems it can model. Therefore, the dynamics of these systems continue to be studied by complexity researchers with only modest progress. <sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v2.pdf?sv=2022-11-02&amp;ss=bf&amp;srt=co&amp;sp=rwdlaciytfx&amp;se=2024-07-31T19:43:39Z&amp;st=2023-07-31T11:43:39Z&amp;spr=https&amp;sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[25]</a></sup><sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v3.pdf?sv=2022-11-02&amp;ss=bf&amp;srt=co&amp;sp=rwdlaciytfx&amp;se=2024-07-31T19:43:39Z&amp;st=2023-07-31T11:43:39Z&amp;spr=https&amp;sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[11]</a></sup>\\nHuman: Can you summarize your last response?\\nAI: In my previous response, I mentioned that reinforcement learning has various use cases, including predictive strategies, social behaviors in multi-robot domains, and model-based learning. Some researchers have studied reinforcement learning in market-based MASs, showing how certain initial learning biases can be self-fulfilling, and how learning can be useful but affected by an agent\\'s models of other agents. However, it is important to note that the behavior of multi-agent systems becomes much harder to predict as we remove limitations from the CLRI framework, which places constraints on the type of systems it can model. Therefore, the dynamics of these systems continue to be studied by complexity researchers with only modest progress <sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v2.pdf?sv=2022-11-02&amp;ss=bf&amp;srt=co&amp;sp=rwdlaciytfx&amp;se=2024-07-31T19:43:39Z&amp;st=2023-07-31T11:43:39Z&amp;spr=https&amp;sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[25]</a></sup><sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v3.pdf?sv=2022-11-02&amp;ss=bf&amp;srt=co&amp;sp=rwdlaciytfx&amp;se=2024-07-31T19:43:39Z&amp;st=2023-07-31T11:43:39Z&amp;spr=https&amp;sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[11]</a></sup>.\\nHuman: Thank you\\nAI: In my previous response, I mentioned that reinforcement learning has various use cases, including predictive strategies, social behaviors in multi-robot domains, and model-based learning. Some researchers have studied reinforcement learning in market-based MASs, showing how certain initial learning biases can be self-fulfilling, and how learning can be useful but affected by an agent\\'s models of other agents. To elaborate, Matarić [22] has studied reinforcement learning in multi-robot domains, noting how learning can give rise to social behaviors [23]. Carmel and Markovitch [4] work on model-based learning, where agents build models of other agents via observations. They use models based on finite state machines. The authors show how some of these models can be effectively learned via observation of the other agent’s actions. There is also experimental work that has been done in the area of agents learning about agents [27, 36]. For example, Sen and Sekaran [28] show how learning agents in simple MAS converge to system-wide optimal behavior. Their agents use Q-learning or modified classifier systems in order to learn. Other researchers have extended the basic Q-learning [35] algorithm for use with MASs in an effort to either improve or prove convergence to the optimal behavior [32, 20, 13]. \\n\\nSources: <sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v2.pdf?sv=2022-11-02&amp;ss=bf&amp;srt=co&amp;sp=rwdlaciytfx&amp;se=2024-07-31T19:43:39Z&amp;st=2023-07-31T11:43:39Z&amp;spr=https&amp;sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[25]</a></sup><sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v3.pdf?sv=2022-11-02&amp;ss=bf&amp;srt=co&amp;sp=rwdlaciytfx&amp;se=2024-07-31T19:43:39Z&amp;st=2023-07-31T11:43:39Z&amp;spr=https&amp;sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[11]</a></sup><sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v2.pdf?sv=2022-11-02&amp;ss=bf&amp;srt=co&amp;sp=rwdlaciytfx&amp;se=2024-07-31T19:43:39Z&amp;st=2023-07-31T11:43:39Z&amp;spr=https&amp;sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[22]</a></sup><sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v3.pdf?sv=2022-11-02&amp;ss=bf&amp;srt=co&amp;sp=rwdlaciytfx&amp;se=2024-07-31T19:43:39Z&amp;st=2023-07-31T11:43:39Z&amp;spr=https&amp;sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[4]</a></sup><sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v3.pdf?sv=2022-11-02&amp;ss=bf&amp;srt=co&amp;sp=rwdlaciytfx&amp;se=2024-07-31T19:43:39Z&amp;st=2023-07-31T11:43:39Z&amp;spr=https&amp;sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[27, 36]</a></sup><sup'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87405173",
   "metadata": {},
   "source": [
    "## Using CosmosDB as persistent memory\n",
    "\n",
    "In previous cell we have added local RAM memory to our chatbot. However, it is not persistent, it gets deleted once the app user's session is terminated. It is necessary then to use a Database for persistent storage of each of the bot user conversations, not only for Analytics and Auditing, but also if we wisg to provide recommendations. \n",
    "\n",
    "Here we will store the conversation history into CosmosDB for future auditing purpose.\n",
    "We will use a class in LangChain use CosmosDBChatMessageHistory, see [HERE](https://python.langchain.com/en/latest/_modules/langchain/memory/chat_message_histories/cosmos_db.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7131daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CosmosDB instance from langchain cosmos class.\n",
    "cosmos = CosmosDBChatMessageHistory(\n",
    "    cosmos_endpoint=os.environ['AZURE_COSMOSDB_ENDPOINT'],\n",
    "    cosmos_database=os.environ['AZURE_COSMOSDB_NAME'],\n",
    "    cosmos_container=os.environ['AZURE_COSMOSDB_CONTAINER_NAME'],\n",
    "    connection_string=os.environ['AZURE_COMOSDB_CONNECTION_STRING'],\n",
    "    session_id=\"Agent-Test-Session\" + str(random.randint(1, 1000)),\n",
    "    user_id=\"Agent-Test-User\" + str(random.randint(1, 1000))\n",
    "    )\n",
    "\n",
    "# prepare the cosmosdb instance\n",
    "cosmos.prepare_cosmos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d87cc7c6-5ef1-4492-b133-9f63a392e223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or Memory Object\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",input_key=\"question\",chat_memory=cosmos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27ceb47a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Reinforcement learning has several use cases, including multi-robot domains, where it can give rise to social behaviors and allow robots to be individually programmed to produce certain group behaviors<sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v2.pdf?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[22]</a></sup>. Reinforcement learning can also be used for model-based learning, where agents build models of other agents via observations, and can be effectively learned via observation of the other agent's actions<sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v2.pdf?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[4]</a></sup>. In addition, reinforcement learning has been studied in market-based MASs, showing how certain initial learning biases can be self-fulfilling, and how learning can be useful but is affected by an agent's models of other agents<sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v2.pdf?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[14, 12]</a></sup>. Finally, reinforcement learning has been used in experimental studies of the behavior of reinforcement learning agents<sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v2.pdf?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[6, 27, 36]</a></sup>."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing using our Question\n",
    "response = get_answer(llm=llm, docs=top_docs, query=QUESTION, language=\"English\", chain_type=chain_type, \n",
    "                        memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a5ff826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I apologize, but I think there was a mistake and I provided you with the wrong response. The response I gave you was related to reinforcement learning, but it does not answer your question. Could you please rephrase your question or provide me with more context?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we add a follow up question:\n",
    "response = get_answer(llm=llm, docs=top_docs, query=FOLLOW_UP_QUESTION, language=\"English\", chain_type=chain_type, \n",
    "                      memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be1620fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I apologize, but I think there was a mistake and I provided you with the wrong response earlier. However, I found some information that might help answer your question. Based on the documents I have, reinforcement learning has several use cases, including multi-robot domains, where it can give rise to social behaviors and allow robots to be individually programmed to produce certain group behaviors<sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v2.pdf?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[22]</a></sup>. Reinforcement learning can also be used for model-based learning, where agents build models of other agents via observations, and can be effectively learned via observation of the other agent's actions<sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v2.pdf?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[4]</a></sup>. In addition, reinforcement learning has been studied in market-based MASs, showing how certain initial learning biases can be self-fulfilling, and how learning can be useful but is affected by an agent's models of other agents<sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v2.pdf?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[14, 12]</a></sup>. Finally, reinforcement learning has been used in experimental studies of the behavior of reinforcement learning agents<sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v2.pdf?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[6, 27, 36]</a></sup>. I hope this information helps. Do you have any other questions?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Another follow up query\n",
    "response = get_answer(llm=llm, docs=top_docs, query=\"Thank you\", language=\"English\", chain_type=chain_type,  \n",
    "                      memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc5ac98",
   "metadata": {},
   "source": [
    "Let's check our Azure CosmosDB to see the whole conversation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1d7688a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Can you summarize your last response?', additional_kwargs={}, example=False),\n",
       " AIMessage(content=\"I apologize, but I'm not sure which response you're referring to. Could you please clarify which response you would like me to summarize?\", additional_kwargs={}, example=False),\n",
       " HumanMessage(content='Tell me some use cases for reinforcement learning?', additional_kwargs={}, example=False),\n",
       " AIMessage(content='Reinforcement learning has several use cases, including multi-robot domains, where it can give rise to social behaviors and allow robots to be individually programmed to produce certain group behaviors<sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v2.pdf?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[22]</a></sup>. Reinforcement learning can also be used for model-based learning, where agents build models of other agents via observations, and can be effectively learned via observation of the other agent\\'s actions<sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v2.pdf?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[4]</a></sup>. In addition, reinforcement learning has been studied in market-based MASs, showing how certain initial learning biases can be self-fulfilling, and how learning can be useful but is affected by an agent\\'s models of other agents<sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v2.pdf?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[14, 12]</a></sup>. Finally, reinforcement learning has been used in experimental studies of the behavior of reinforcement learning agents<sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v2.pdf?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[6, 27, 36]</a></sup>.', additional_kwargs={}, example=False),\n",
       " HumanMessage(content='Can you summarize your last response?', additional_kwargs={}, example=False),\n",
       " AIMessage(content='I apologize, but I think there was a mistake and I provided you with the wrong response. The response I gave you was related to reinforcement learning, but it does not answer your question. Could you please rephrase your question or provide me with more context?', additional_kwargs={}, example=False),\n",
       " HumanMessage(content='Thank you', additional_kwargs={}, example=False),\n",
       " AIMessage(content='I apologize, but I think there was a mistake and I provided you with the wrong response earlier. However, I found some information that might help answer your question. Based on the documents I have, reinforcement learning has several use cases, including multi-robot domains, where it can give rise to social behaviors and allow robots to be individually programmed to produce certain group behaviors<sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v2.pdf?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[22]</a></sup>. Reinforcement learning can also be used for model-based learning, where agents build models of other agents via observations, and can be effectively learned via observation of the other agent\\'s actions<sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v2.pdf?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[4]</a></sup>. In addition, reinforcement learning has been studied in market-based MASs, showing how certain initial learning biases can be self-fulfilling, and how learning can be useful but is affected by an agent\\'s models of other agents<sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v2.pdf?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[14, 12]</a></sup>. Finally, reinforcement learning has been used in experimental studies of the behavior of reinforcement learning agents<sup><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v2.pdf?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\" target=\"_blank\">[6, 27, 36]</a></sup>. I hope this information helps. Do you have any other questions?', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load message from cosmosdb\n",
    "cosmos.load_messages()\n",
    "cosmos.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e30694-ae2a-47bb-a5c7-db51ecdbba1e",
   "metadata": {},
   "source": [
    "![CosmosDB Memory](./images/cosmos-chathistory.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6789cada-23a3-451a-a91a-0906ceb0bd14",
   "metadata": {},
   "source": [
    "# Summary\n",
    "##### Adding memory to our application allows the user to have a conversation, however this feature is not something that comes with the LLM, but instead, memory is something that we must provide to the LLM in form of context of the question.\n",
    "\n",
    "We added persitent memory using CosmosDB.\n",
    "\n",
    "We also can notice that the current chain that we are using is smart, but not that much. Although we have given memory to it, it searches for similar docs everytime, it struggles to respond to prompts like: Hello, Thank you, Bye, What's your name, What's the weather and any other task that is not search in the knowledge base.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c629ebf4-aced-45b7-a6a2-315810d37d48",
   "metadata": {},
   "source": [
    "# NEXT\n",
    "We know now how to do a Smart Search Engine that can power a chatbot!! great!\n",
    "\n",
    "But, does this solve all the possible scenarios that a virtual assistant will require?  **What about if the answer to the Smart Search Engine is not related to text, but instead requires to look into tabular data?** The next notebook 05 explains and solves the tabular problem and the concept of Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4d9da4-3918-4da6-b235-a3320f0dcb12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
