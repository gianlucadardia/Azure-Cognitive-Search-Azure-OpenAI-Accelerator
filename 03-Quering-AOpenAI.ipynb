{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d59d527f-1100-45ff-b051-5f7c9029d94d",
   "metadata": {},
   "source": [
    "# Queries with and without Azure OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9a9444-dc90-4fc3-aea7-8ee918301aba",
   "metadata": {},
   "source": [
    "Now that we have our Search Engine loaded **from two different data sources in two diferent indexes**, we are going to try some example queries and then use Azure OpenAI service to see if we can get even better results.\n",
    "\n",
    "The idea is that a user can ask a question about Computer Science (first datasource/index) or about Covid (second datasource/index), and the engine will respond accordingly.\n",
    "This **Multi-Index** demo, mimics the scenario where a company loads multiple type of documents of different types and about completly different topics and the search engine must respond with the most relevant results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f6c7e3-9037-4b1e-ae17-1deaa27b9c08",
   "metadata": {},
   "source": [
    "## Set up variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e50b404-a061-49e7-a3c7-c6eabc98ff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import requests\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from IPython.display import display, HTML\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "from common.prompts import COMBINE_QUESTION_PROMPT, COMBINE_PROMPT\n",
    "from common.utils import model_tokens_limit, num_tokens_from_docs\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"credentials.env\")\n",
    "\n",
    "# Demo Datasource Blob Storage. Change if using your own data\n",
    "#DATASOURCE_SAS_TOKEN = \"?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2023-11-29T01:50:59Z&st=2023-05-10T16:50:59Z&spr=https&sig=ZT7MLy%2BnlvAxUKKj5v0RwoWObXaab3gO4ec2%2Fci6iL0%3D\"\n",
    "\n",
    "#Personal storage data source\n",
    "DATASOURCE_SAS_TOKEN = \"?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f2c22f8-79ab-405c-95e8-77a1978e53bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Payloads header\n",
    "headers = {'Content-Type': 'application/json','api-key': os.environ['AZURE_SEARCH_KEY']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9297d29b-1f61-4dce-858e-bf4272172dba",
   "metadata": {},
   "source": [
    "## Multi-Index Search queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a46e2d3-298a-4708-83de-9e108b1a117a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Index that we are going to query (from Notebook 01 and 02)\n",
    "index1_name = \"cogsrch-index-files\"\n",
    "index2_name = \"cogsrch-index-csv\"\n",
    "indexes = [index1_name, index2_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c62ebb2-d7be-4bfb-b1ba-4db86c11839a",
   "metadata": {},
   "source": [
    "Try questions that you think might be answered or addressed in computer science papers in 2020-2021 or that can be addressed by medical publications about COVID in 2020-201. Try comparing the results with the open version of ChatGPT.<br>\n",
    "The idea is that the answers using Azure OpenAI only looks at the information contained on these publications.\n",
    "\n",
    "**Example Questions you can ask**:\n",
    "- What is CLP?\n",
    "- How Markov chains work?\n",
    "- What are some examples of reinforcement learning?\n",
    "- What are the main risk factors for Covid-19?\n",
    "- What medicine reduces inflamation in the lungs?\n",
    "- Why Covid doesn't affect kids that much compared to adults?\n",
    "- Does chloroquine really works against covid?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9b53c14-19bd-451f-aa43-7ad27ccfeead",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"What is CLP?\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d925eb-7f9c-429e-a62a-4c37d7702caf",
   "metadata": {},
   "source": [
    "### Search on both indexes individually and aggragate results\n",
    "\n",
    "#### **Note**: \n",
    "In order to standarize the indexes, **there must be 7 mandatory fields present on each index**: `id, title, content, chunks, language, name, location`. This is so that each document can be treated the same along the code. Also, **all indexes must have a semantic configuration**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faf2e30f-e71f-4533-ab52-27d048b80a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://cog-search-basictier.search.windows.net/indexes/cogsrch-index-files/docs?api-version=2023-07-01-Preview&search=What is CLP?&select=id,title,chunks,language,name,location&$top=10&queryLanguage=en-us&queryType=semantic&semanticConfiguration=my-semantic-config&$count=true&speller=lexicon&answers=extractive|count-3&captions=extractive|highlight-false\n",
      "200\n",
      "Results Found: 31, Results Returned: 10\n",
      "https://cog-search-basictier.search.windows.net/indexes/cogsrch-index-csv/docs?api-version=2023-07-01-Preview&search=What is CLP?&select=id,title,chunks,language,name,location&$top=10&queryLanguage=en-us&queryType=semantic&semanticConfiguration=my-semantic-config&$count=true&speller=lexicon&answers=extractive|count-3&captions=extractive|highlight-false\n",
      "200\n",
      "Results Found: 6106, Results Returned: 10\n"
     ]
    }
   ],
   "source": [
    "agg_search_results = []\n",
    "\n",
    "for index in indexes:\n",
    "    url = os.environ['AZURE_SEARCH_ENDPOINT'] + '/indexes/'+ index + '/docs'\n",
    "    url += '?api-version={}'.format(os.environ['AZURE_SEARCH_API_VERSION'])\n",
    "    url += '&search={}'.format(QUESTION)\n",
    "    url += '&select=id,title,chunks,language,name,location'\n",
    "    url += '&$top=10'  # You can change this to anything you need/want\n",
    "    url += '&queryLanguage=en-us'\n",
    "    url += '&queryType=semantic'\n",
    "    url += '&semanticConfiguration=my-semantic-config'\n",
    "    url += '&$count=true'\n",
    "    url += '&speller=lexicon'\n",
    "    url += '&answers=extractive|count-3'\n",
    "    url += '&captions=extractive|highlight-false'\n",
    "\n",
    "    resp = requests.get(url, headers=headers)\n",
    "    print(url)\n",
    "    print(resp.status_code)\n",
    "\n",
    "    search_results = resp.json()\n",
    "    agg_search_results.append(search_results)\n",
    "    print(\"Results Found: {}, Results Returned: {}\".format(search_results['@odata.count'], len(search_results['value'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fd0fe5-4ee0-42e2-a920-72b93a407389",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Display the top results (from both searches) based on the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e938337-602d-4b61-8141-b8c92a5d91da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Top Answers</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5>Answer - score: 0.72</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "In vitro assembly of alphavirus nucleocapsid cores, called core-like particles (CLPs), requires a polyanionic cargo. There are no sequence or structure requirements to encapsidate single-stranded nucleic acid cargo. In this work, we wanted to determine how the length of the cargo impacts the stability and structure of the assembled CLPs."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h4>Top Results</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7103146/?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\">Length of encapsidated cargo impacts stability and structure of in vitro assembled alphavirus core-like particles</a> - score: 2.49</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "In vitro assembly of alphavirus nucleocapsid cores, called core-like particles (CLPs), requires a polyanionic cargo. There are no sequence or structure requirements to encapsidate single-stranded nucleic acid cargo. In this work, we wanted to determine how the length of the cargo impacts the stability and structure of the assembled CLPs."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001009v1.pdf?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\">0001009v1.pdf</a> - score: 2.3</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Fractal symbolic analysis com- pares a program and its transformed version by repeat- edly simplifying these programs until symbolic analysis becomes tractable, ensuring that equality of simplified programs is sufficient to guarantee equality of the origi- nal programs."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5809586/?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\">A cold-inducible RNA-binding protein (CIRP)-derived peptide attenuates inflammation and organ injury in septic mice</a> - score: 2.05</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Cold-inducible RNA-binding protein (CIRP) is a novel sepsis inflammatory mediator and C23 is a putative CIRP competitive inhibitor. Therefore, we hypothesized that C23 can ameliorate sepsis-associated injury to the lungs and kidneys."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6957694/?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\">Rolipram Protects Mice from Gram-negative Bacterium Escherichia coli-induced Inflammation and Septic Shock</a> - score: 1.63</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "We found that rolipram significantly improves survival in mice challenged with gram-negative bacterium E. coli, CLP, or E. coli derived lipopolysaccharide. We have also found that rolipram inhibits organ damage, pro-inflammatory cytokine production, and intracellular migration of early-stage inflammatory elements."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7099173/?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\">Protective Effect of Dexmedetomidine on Acute Lung Injury via the Upregulation of Tumour Necrosis Factor-α-Induced Protein-8-like 2 in Septic Mice</a> - score: 1.62</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Control mice were infected with an adeno-associated virus expressing no transgene. Three weeks later, an animal model of caecal ligation-perforation (CLP)-induced sepsis was established. DEX was administered intravenously 30 min after CLP."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6016888/?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\">B-1a cells protect mice from sepsis-induced acute lung injury</a> - score: 1.44</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "We hypothesize that B-1a cells ameliorate sepsis-induced ALI. METHODS: Sepsis was induced in C57BL/6 mice by cecal ligation and puncture (CLP). PBS or B-1a cells were adoptively transferred into the septic mice intraperitoneally."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4243715/?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\">Silencing airway epithelial cell-derived hepcidin exacerbates sepsis-induced acute lung injury</a> - score: 1.39</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INTRODUCTION: The production of antimicrobial peptides by airway epithelial cells is an important component of the innate immune response to pulmonary infection and inflammation. Hepcidin is a β-defensin-like antimicrobial peptide and acts as a principal iron regulatory hormone."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7102048/?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\">IL-27 is Elevated in Acute Lung Injury and Mediates Inflammation</a> - score: 1.31</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Besides, a mouse model of cecal ligation and puncture (CLP)-induced lung inflammation/injury was established, and serum, BAL fluid and tissues were collected for analyses in the presence or absence of IL-27 neutralizing antibodies."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7070710/?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\">Amyloidogenic Peptides in Human Neuro-Degenerative Diseases and in Microorganisms: A Sorrow Shared Is a Sorrow Halved?</a> - score: 1.2</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Common to these peptides is a stable cross-β dominated secondary structure which allows self-assembly, leading to insoluble oligomers and lastly to fibrils. These highly ordered protein aggregates have been, for a long time, mainly associated with human neurodegenerative diseases such as Alzheimer’s disease (Amyloid-β peptides)."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001015v1.pdf?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\">arXiv:cs/0001015v1  [cs.AI]  19 Jan 2000</a> - score: 1.15</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "• Second, the set of conceivable worlds—the union of the set of “possible” worlds considered when evaluating L and the set of “impossible” worlds considered when evaluating N—is fixed, independent of the situation (W,w); it is always the set of all truth assignments."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7097046/?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\">What does the future hold for clinical microbiology?</a> - score: 1.06</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The combination of the new tools that are now being developed in research laboratories, the general reorganization of clinical laboratories and improved communication between physicians and clinical microbiologists should lead to profound changes in the way that clinical microbiologists work..\u0000"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001014v2.pdf?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\">arXiv:cs/0001014v2  [cs.CC]  21 Apr 2000</a> - score: 1.04</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "This is a polynomial p such that p(x) 6= 0 iff f(x) = 1. Let the non-deterministic degree of f , de- noted ndeg(f), be the minimum degree among all non- deterministic polynomials p for f . Without loss of gener- ality we can assume p(x) ∈ [−1, 1] for all x ∈ {0, 1}n (if not, just divide by maxx |p(x)|)."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5858307/?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\">Near-Patient Sampling to Assist Infection Control—A Case Report and Discussion</a> - score: 1.01</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "This case report used a button sampler, worn or held by staff or left free-standing in a fixed location, for environmental sampling around a child who was chronically infected by a respiratory adenovirus, to determine whether there was any risk of secondary adenovirus infection to the staff managing the patient."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001021v1.pdf?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\">arXiv:cs/0001021v1  [cs.CL]  24 Jan 2000</a> - score: 0.93</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The model, its probabilistic parametrization, a reestimation algorithm for the model pa- rameters and a set of experiments meant to evaluate its potential for speech recognition are presented.  1 INTRODUCTION The task of a speech recognizer is to automatically transcribe speech into text."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001022v1.pdf?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\">arXiv:cs/0001022v1  [cs.CL]  24 Jan 2000</a> - score: 0.92</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "a∗ for lattice decoding there are a couple of reasons that make a∗ appealing for our problem: • the algorithm operates with whole prefixes x, making it ideal for incorporating language models whose memory is the entire prefix; • a reasonably good overestimate h(y|x) and an efficient way to calculate hl(x) (see eq.6) are readily available using …"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001004v1.pdf?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\">arXiv:cs/0001004v1  [cs.LG]  7 Jan 2000</a> - score: 0.91</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The prewhitening is a linear transformation of the observed data which maps the covariance matrix to the unit matrix. If we deal with prewhitened data, we can legitimately narrow the sweeping range to the orthogonal group. The aim of this letter is the construction of a multiplicative algorithm for the orthogonal groups."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v3.pdf?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\">Predicting the Expected Behavior of Agents that Learn About Agents: The CLRI Framework</a> - score: 0.9</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "A difference equation is used for calculating the progression of an agent’s error in its decision function, thereby telling us how the agent is expected to fare in the MAS."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001020v1.pdf?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\">arXiv:cs/0001020v1  [cs.CL]  24 Jan 2000</a> - score: 0.86</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "A maximum likelihood rees-  timation procedure belonging to the class of expectation-maximization algorithms is  employed for training the model."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v2.pdf?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\">arXiv:cs/0001008v2  [cs.MA]  17 Jan 2000</a> - score: 0.75</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "A difference equation is used for calculating the progression of an agent’s error in its decision function, thereby telling us how the agent is expected to fare in the MAS."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h5><a href=\"https://gdoaisa.blob.core.windows.net/arxivcs/0001008v1.pdf?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\">arXiv:cs/0001008v1  [cs.MA]  12 Jan 2000</a> - score: 0.73</h5>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "A difference equation is used for calculating the progression of an agent’s error in its decision function, thereby telling us how the agent is expected to fare in the MAS."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML('<h4>Top Answers</h4>'))\n",
    "\n",
    "for search_results in agg_search_results:\n",
    "    for result in search_results['@search.answers']:\n",
    "        if result['score'] > 0.5: # Show answers that are at least 50% of the max possible score=1\n",
    "            display(HTML('<h5>' + 'Answer - score: ' + str(round(result['score'],2)) + '</h5>'))\n",
    "            display(HTML(result['text']))\n",
    "            \n",
    "print(\"\\n\\n\")\n",
    "display(HTML('<h4>Top Results</h4>'))\n",
    "\n",
    "content = dict()\n",
    "ordered_content = OrderedDict()\n",
    "\n",
    "\n",
    "for search_results in agg_search_results:\n",
    "    for result in search_results['value']:\n",
    "        content[result['id']]={\n",
    "                                \"title\": result['title'],\n",
    "                                \"chunks\": result['chunks'],\n",
    "                                \"language\": result['language'], \n",
    "                                \"name\": result['name'], \n",
    "                                \"location\": result['location'] ,\n",
    "                                \"caption\": result['@search.captions'][0]['text'],\n",
    "                                \"score\": result['@search.rerankerScore']               \n",
    "                                }\n",
    "    \n",
    "#After results have been filtered we will Sort and add them as an Ordered list\\n\",\n",
    "for id in sorted(content, key= lambda x: content[x][\"score\"], reverse=True):\n",
    "    ordered_content[id] = content[id]\n",
    "    url = str(ordered_content[id]['location']) + DATASOURCE_SAS_TOKEN\n",
    "    title = str(ordered_content[id]['title']) if (ordered_content[id]['title']) else ordered_content[id]['name']\n",
    "    score = str(round(ordered_content[id]['score'],2))\n",
    "    display(HTML('<h5><a href=\"'+ url + '\">' + title + '</a> - score: '+ score + '</h5>'))\n",
    "    display(HTML(ordered_content[id]['caption']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a6d3e6-afb2-4fa7-96d3-69bc2373ded5",
   "metadata": {},
   "source": [
    "## Comments on Query results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e02227-6a92-4944-86f8-6c1e38d90fe4",
   "metadata": {},
   "source": [
    "As seen above the semantic search feature of Azure Cognitive Search service is good. It gives us some answers and also the top results with the corresponding file and the paragraph where the answers is possible located.\n",
    "\n",
    "Let's see if we can make this better with Azure OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df3e6d4-9a09-4b0f-b328-238738ccfaec",
   "metadata": {},
   "source": [
    "# Using Azure OpenAI\n",
    "\n",
    "To use OpenAI to get a better answer to our question, the thought process is: let's send the the documents of the search result to the GPT model and let it understand the document's content and provide a better response.\n",
    "\n",
    "We will use a genius library call **LangChain** that wraps a lot of boiler plate code.\n",
    "Langchain is one library that does a lot of the prompt engineering for us under the hood, for more information see [here](https://python.langchain.com/en/latest/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7c720e-ece1-45ad-9d01-2dfd15c182bb",
   "metadata": {},
   "source": [
    "## A gentle intro to chaining LLMs and prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcd7028-5a6c-4296-8c85-4f420d408d69",
   "metadata": {},
   "source": [
    "Chains are what you get by connecting one or more large language models (LLMs) in a logical way. (Chains can be built of entities other than LLMs but for now, let’s stick with this definition for simplicity).\n",
    "\n",
    "Azure OpenAI is a type of LLM (provider) that you can use but there are others like Cohere, Huggingface, etc.\n",
    "\n",
    "Chains can be simple (i.e. Generic) or specialized (i.e. Utility).\n",
    "\n",
    "* Generic — A single LLM is the simplest chain. It takes an input prompt and the name of the LLM and then uses the LLM for text generation (i.e. output for the prompt).\n",
    "\n",
    "Here’s an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eea62a7d-7e0e-4a93-a89c-20c96560c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
    "os.environ[\"OPENAI_API_BASE\"] = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "os.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13df9247-e784-4e04-9475-55e672efea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our LLM model\n",
    "# Make sure you have the deployment named \"gpt-35-turbo\" for the model \"gpt-35-turbo (0301)\". \n",
    "# Use \"gpt-4\" if you have it available.\n",
    "MODEL = \"gpt-35-turbo\" # options: gpt-35-turbo, gpt-4, gpt-4-32k\n",
    "llm = AzureChatOpenAI(deployment_name=MODEL, temperature=0, max_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b0520b9-83b2-49fd-ad84-624cb0f15ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following question: \"What is CLP?\". Give your response in French\n"
     ]
    }
   ],
   "source": [
    "# Now we create a simple prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"language\"],\n",
    "    template='Answer the following question: \"{question}\". Give your response in {language}',\n",
    ")\n",
    "\n",
    "print(prompt.format(question=QUESTION, language=\"French\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcc7dae3-6b88-4ea6-be43-b178ebc559dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is CLP?',\n",
       " 'language': 'French',\n",
       " 'text': \"CLP signifie Classification, Étiquetage et Emballage des substances et des mélanges. Il s'agit d'un système de classification et d'étiquetage harmonisé au niveau européen pour les produits chimiques dangereux. Le CLP vise à protéger la santé humaine et l'environnement en fournissant des informations claires et précises sur les dangers des produits chimiques.\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And finnaly we create our first generic chain\n",
    "chain_chat = LLMChain(llm=llm, prompt=prompt)\n",
    "chain_chat({\"question\": QUESTION, \"language\": \"French\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ed014c-0c6b-448c-b995-fe7970b92ad5",
   "metadata": {},
   "source": [
    "Great!!, now you know how to create a simple prompt and use a chain in order to answer a general question using ChatGPT knowledge!. \n",
    "\n",
    "It is important to note that we rarely use generic chains as standalone chains. More often they are used as building blocks for Utility chains (as we will see next). Also important to notice is that we are NOT using our documents or the result of the Azure Search yet, just the knowledge of ChatGPT on the data it was trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c48038-b1af-4228-8ffb-720e554fd3b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "**The second type of Chains are Utility:**\n",
    "\n",
    "* Utility — These are specialized chains, comprised of many LLMs to help solve a specific task. For example, LangChain supports some end-to-end chains (such as [QA_WITH_SOURCES](https://python.langchain.com/en/latest/modules/chains/index_examples/qa_with_sources.html) for QnA Doc retrieval, Summarization, etc) and some specific ones (such as GraphQnAChain for creating, querying, and saving graphs). \n",
    "\n",
    "We will look at one specific chain called **qa_with_sources** in this workshop for digging deeper and solve our use case of enhancing the results of Azure Cognitive Search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0454ddb-44d8-4fa9-929a-5e5563dd28f8",
   "metadata": {},
   "source": [
    "\n",
    "But before dealing with the utility chain needed, we need to deal first with this problem: **the content of the search result files is or can be very lengthy, more than the allowed tokens allowed by the GPT Azure OpenAI models**. So what we need to do is: split in chunks, vectorize those chunks and do a vector semantic search to get the top chunks in order to provide the best and not too lenghy context to the LLM.\n",
    "\n",
    "Notice that **the documents chunks are already done in Azure Search**. *ordered_content* dictionary (created a few cells above) contains the pages (chunks) of each document. So we don't really need to chunk them again, but we still need to make sure that we can be as fast as possible and that we are below the max allowed input token limits of our selected OpenAI model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f7b41d2-65b0-4058-8a46-c76cf6960720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 113\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each of the results chunks and create a LangChain Document class to use further in the pipeline\n",
    "docs = []\n",
    "for key,value in ordered_content.items():\n",
    "    for page in value[\"chunks\"]:\n",
    "        location = value[\"location\"] if value[\"location\"] is not None else \"\"\n",
    "        docs.append(Document(page_content=page, metadata={\"source\": location+DATASOURCE_SAS_TOKEN}))\n",
    "        \n",
    "print(\"Number of chunks:\",len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345d35f6-b7c8-4fda-a9e2-94a7da16a18e",
   "metadata": {},
   "source": [
    "We need now to calculate the number of tokens for all the chunks combined to decide what to do:\n",
    "1) Should we embed to vectors and do cosine similarity because there is too much data to fit on the prompt as context?\n",
    "2) What happens if the amount of chunks is too big? how do we keep the response time on-check?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62bd5169-f273-4c66-a91b-6de990dad244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom token limit for gpt-35-turbo : 2500\n",
      "Combined docs tokens count: 144394\n"
     ]
    }
   ],
   "source": [
    "# Calculate number of tokens of our docs\n",
    "if(len(docs)>0):\n",
    "    tokens_limit = model_tokens_limit(MODEL) # this is a custom function we created in common/utils.py\n",
    "    num_tokens = num_tokens_from_docs(docs) # this is a custom function we created in common/utils.py\n",
    "    print(\"Custom token limit for\", MODEL, \":\", tokens_limit)\n",
    "    print(\"Combined docs tokens count:\",num_tokens)\n",
    "        \n",
    "else:\n",
    "    print(\"NO RESULTS FROM AZURE SEARCH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5403dee-a4c4-420c-9819-68151d973695",
   "metadata": {},
   "source": [
    "Now, depending of the amount of chunks/pages returned from the search result, which is very related to the size of the documents returned, we need to make some decisions to keep the speed of the response from Azure OpenAI at reasonable levels.\n",
    "\n",
    "**The logic is**: if there is less than X chunks (of 5000 chars each) to vectorize, then we use OpenAI models in all of them, which currently don't offer batch processing (but it will soon), but if there is more than X chunks we have to trim the number of chunks to the first X, otherwise large documents can take minutes to vectorize.\n",
    "\n",
    "**Note**, this is a temporary solution. Once Vector Capabilities in Azure Cognitive Search is in Public Preview, then we can vectorize once, store, then retrieve them again when needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a03f1f10-32b0-4c1e-8a0e-eee1b1d29ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Number of chunks: 113\n",
      "Truncated Number of chunks: 100\n",
      "Token count after similarity search: 1333\n",
      "Chain Type selected: stuff\n",
      "CPU times: total: 1.48 s\n",
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "chunks_limit = 100 # This is the limit of how many chunks are we willing to take in order to keep the response fairly quick\n",
    "\n",
    "if num_tokens > tokens_limit:\n",
    "    # Select the Embedder model\n",
    "    embedder = OpenAIEmbeddings(deployment=\"text-embedding-ada-002\", chunk_size=1) \n",
    "    print(\"Initial Number of chunks:\",len(docs))\n",
    "    if len(docs) > chunks_limit:\n",
    "        docs = docs[:chunks_limit]\n",
    "        print(\"Truncated Number of chunks:\",len(docs))\n",
    "    \n",
    "    # Create our in-memory vector database index from the chunks given by Azure Search.\n",
    "    # We are using FAISS. https://ai.facebook.com/tools/faiss/\n",
    "    db = FAISS.from_documents(docs, embedder)\n",
    "    top_docs = db.similarity_search(QUESTION, k=4)  # Return the top 4 documents\n",
    "    \n",
    "    # Now we need to recalculate the tokens count of the top results from similarity vector search\n",
    "    # in order to select the chain type: stuff (all chunks in one prompt) or \n",
    "    # map_reduce (multiple calls to the LLM to summarize/reduce the chunks and then combine them)\n",
    "    \n",
    "    num_tokens = num_tokens_from_docs(top_docs)\n",
    "    print(\"Token count after similarity search:\", num_tokens)\n",
    "    chain_type = \"map_reduce\" if num_tokens > tokens_limit else \"stuff\"\n",
    "    \n",
    "else:\n",
    "    # if total tokens is less than our limit, we don't need to vectorize and do similarity search\n",
    "    top_docs = docs\n",
    "    chain_type = \"stuff\"\n",
    "    \n",
    "print(\"Chain Type selected:\", chain_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17247488-7d14-4178-9add-31eb1afcbcbe",
   "metadata": {},
   "source": [
    "At this point we already have the top most similar chunks (in order of relevance) in **top_docs**\n",
    "\n",
    "Now we need Azure OpenAI GPT model to understand these top chunks and provide us an answer to the question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8c6ad1-82b8-4fd8-80a3-0276b81d7231",
   "metadata": {},
   "source": [
    "For this task, we need to come back to the Utility Chain: **qa_with_sources** that we mentioned before. See [HERE](https://python.langchain.com/en/latest/modules/chains/index_examples/qa_with_sources.html) for reference.\n",
    "\n",
    "We created our own custom prompts so we can add translation to a specified language. But, for more information on the different types of prompts for this utility chain please see [HERE](https://github.com/hwchase17/langchain/tree/master/langchain/chains/question_answering)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ab16c86-9863-4001-89af-6819c6f3240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if chain_type == \"stuff\":\n",
    "    chain = load_qa_with_sources_chain(llm, chain_type=chain_type, \n",
    "                                       prompt=COMBINE_PROMPT)\n",
    "elif chain_type == \"map_reduce\":\n",
    "    chain = load_qa_with_sources_chain(llm, chain_type=chain_type, \n",
    "                                       question_prompt=COMBINE_QUESTION_PROMPT,\n",
    "                                       combine_prompt=COMBINE_PROMPT,\n",
    "                                       return_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28926219-74c2-4538-8493-129463ac40a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the below line if you want to check our custom COMBINE_PROMPT\n",
    "# print(chain.combine_document_chain.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1e619b8-1dcf-431b-8aad-f1696a09c2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 422 ms\n",
      "Wall time: 5.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Try with other language as well\n",
    "response = chain({\"input_documents\": top_docs, \"question\": QUESTION, \"language\": \"English\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "681fe1de-e37c-4355-accc-6dd15b6a310a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "CLP can refer to different things depending on the context. In the context of the following documents, CLP can mean: Consultation-Liaison Psychiatry<sup><a href=\"https://api.elsevier.com/content/article/pii/S0033318220301420; https://www.sciencedirect.com/science/article/pii/S0033318220301420?v=s5?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D</a></sup>, monocyte chemoattractant protein 1/CC chemokine ligand 2<sup><a href=\"https://www.ncbi.nlm.nih.gov/pubmed/17047515/?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\">[2]</a></sup>, and core-like particles<sup><a href=\"https://www.ncbi.nlm.nih.gov/pubmed/10403670/?sv=2022-11-02&ss=bf&srt=co&sp=rwdlaciytfx&se=2024-07-31T19:43:39Z&st=2023-07-31T11:43:39Z&spr=https&sig=59%2BzIeHnln8hbizI2AUj%2FKJjcZhyiS1NrZJd8mVFIWA%3D\">[3]</a></sup><sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7103146/?sv=2022-11-02&ss=bf"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(response['output_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11345374-6420-4b36-b061-795d2a804c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you want to inspect the results from map_reduce chain type, each top similar chunk summary (k=4 by default)\n",
    "\n",
    "# if chain_type == \"map_reduce\":\n",
    "#     for step in response['intermediate_steps']:\n",
    "#         display(HTML(\"<b>Chunk Summary:</b> \" + step))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f347373a-a5be-473d-b64e-0f6b6dbcd0e0",
   "metadata": {},
   "source": [
    "# Summary\n",
    "##### This answer is way better than taking just the result from Azure Cognitive Search. So the summary is:\n",
    "- Azure Cognitive Search give us the top results (context)\n",
    "- Azure OpenAI takes these results and understand the content and uses it as context to give the best answer\n",
    "- Best of two worlds!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc6e2fe-1c34-4952-99ad-14940f022379",
   "metadata": {},
   "source": [
    "# NEXT\n",
    "We just added a smart layer on top of Azure Cognitive Search. This is the backend for a GPT Smart Search Engine.\n",
    "\n",
    "However, we are missing something: **How to have a conversation with this engine?**\n",
    "\n",
    "On the next Notebook, we are going to understand the concept of **memory**. This is necessary in order to have a chatbot that can establish a conversation with the user. Without memory, there is no real conversation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
