{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a8b5c0-87cb-4302-8e3c-dc809d0039fb",
   "metadata": {},
   "source": [
    "# Understanding Memory in LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f73380-6395-4e9f-9c83-3f47a5d7e292",
   "metadata": {},
   "source": [
    "In the previous Notebook, we successfully explored how OpenAI models can enhance the results from Azure Cognitive Search. \n",
    "\n",
    "However, we have yet to discover how to engage in a conversation with the LLM. With [Bing Chat](http://chat.bing.com/), for example, this is possible, as it can understand and reference the previous responses.\n",
    "\n",
    "There is a common misconception that GPT models have memory. This is not true. While they possess knowledge, they do not retain information from previous questions asked to them.\n",
    "\n",
    "In this Notebook, our goal is to illustrate how we can effectively \"endow the LLM with memory\" by employing prompts and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "733c782e-204c-47d0-8dae-c9df7091ab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from openai.error import OpenAIError\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.memory import CosmosDBChatMessageHistory\n",
    "\n",
    "from IPython.display import Markdown, HTML, display  \n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "\n",
    "#custom libraries that we will use later in the app\n",
    "from common.utils import (\n",
    "    get_search_results,\n",
    "    update_vector_indexes,\n",
    "    model_tokens_limit,\n",
    "    num_tokens_from_docs,\n",
    "    num_tokens_from_string,\n",
    "    get_answer,\n",
    ")\n",
    "\n",
    "from common.prompts import COMBINE_CHAT_PROMPT_TEMPLATE\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"credentials.env\")\n",
    "\n",
    "import logging\n",
    "\n",
    "# Get the root logger\n",
    "logger = logging.getLogger()\n",
    "# Set the logging level to a higher level to ignore INFO messages\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bc63c55-a57d-49a7-b6c7-0f18bca8199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
    "os.environ[\"OPENAI_API_BASE\"] = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ[\"AZURE_OPENAI_API_KEY\"]\n",
    "os.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc72b22-11c2-4df0-91b8-033d01829663",
   "metadata": {},
   "source": [
    "### Let's start with the basics\n",
    "Let's use a very simple example to see if the GPT model of Azure OpenAI have memory. We again will be using langchain to simplify our code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eef5dc9-8b80-4085-980c-865fa41fa1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Tell me some use cases for reinforcement learning?\"\n",
    "FOLLOW_UP_QUESTION = \"Give me the main points of our conversation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a00181d5-bd76-4ce4-a256-75ac5b58c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "MODEL = \"gpt-35-turbo\"\n",
    "COMPLETION_TOKENS = 500\n",
    "# Create an OpenAI instance\n",
    "llm = AzureChatOpenAI(deployment_name=MODEL, temperature=0.5, max_tokens=COMPLETION_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9502d0f1-fddf-40d1-95d2-a1461dcc498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a very simple prompt template, just the question as is:\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"{question}\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5c9903e-15c7-4e05-87a1-58e5a7917ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. Robotics: Reinforcement learning can be used to train robots to perform complex tasks such as object manipulation, navigation, and grasping.\n",
       "\n",
       "2. Game playing: Reinforcement learning has been successfully used to train game-playing agents, such as AlphaGo, which defeated the world champion in the game of Go.\n",
       "\n",
       "3. Autonomous vehicles: Reinforcement learning can be used to train autonomous vehicles to make decisions in real-time, such as when to accelerate, brake, or change lanes.\n",
       "\n",
       "4. Natural language processing: Reinforcement learning can be used to train chatbots and virtual assistants to understand and respond to natural language queries.\n",
       "\n",
       "5. Finance: Reinforcement learning can be used to optimize investment portfolios, detect fraud, and predict market trends.\n",
       "\n",
       "6. Healthcare: Reinforcement learning can be used to develop personalized treatment plans for patients based on their medical history and symptoms.\n",
       "\n",
       "7. Advertising: Reinforcement learning can be used to optimize ad placement and targeting to maximize revenue for advertisers.\n",
       "\n",
       "8. Supply chain management: Reinforcement learning can be used to optimize inventory management, demand forecasting, and logistics planning.\n",
       "\n",
       "9. Energy management: Reinforcement learning can be used to optimize energy consumption in buildings and power grids.\n",
       "\n",
       "10. Agriculture: Reinforcement learning can be used to optimize crop yield, reduce waste, and improve sustainability in agriculture."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's see what the GPT model responds\n",
    "response = chain.run(QUESTION)\n",
    "printmd(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99acaf3c-ce68-4b87-b24a-6065b15ff9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but as an AI language model, I do not have access to any previous conversation that you had with someone else. Can you please provide more context or clarify which conversation you are referring to?\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now let's ask a follow up question\n",
    "chain.run(FOLLOW_UP_QUESTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e1c143-c95f-4566-a8b4-af8c42f08dd2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "As you can see, it doesn't remember what it just responded, sometimes it responds based only on the system prompt, or just randomly. This proof that the LLM does NOT have memory and that we need to give the memory as a a conversation history as part of the prompt, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0946ce71-6285-432e-b011-9c2dc1ba7b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"question\"],\n",
    "    template=\"\"\"\n",
    "                {history}\n",
    "                Human: {question}\n",
    "                AI:\n",
    "            \"\"\"\n",
    "    )\n",
    "chain = LLMChain(llm=llm, prompt=hist_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d088e51-e5eb-4143-b87d-b2be429eb864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Human: Tell me some use cases for reinforcement learning?\n",
      "AI: 1. Robotics: Reinforcement learning can be used to train robots to perform complex tasks such as object manipulation, navigation, and grasping.\n",
      "\n",
      "2. Game playing: Reinforcement learning has been successfully used to train game-playing agents, such as AlphaGo, which defeated the world champion in the game of Go.\n",
      "\n",
      "3. Autonomous vehicles: Reinforcement learning can be used to train autonomous vehicles to make decisions in real-time, such as when to accelerate, brake, or change lanes.\n",
      "\n",
      "4. Natural language processing: Reinforcement learning can be used to train chatbots and virtual assistants to understand and respond to natural language queries.\n",
      "\n",
      "5. Finance: Reinforcement learning can be used to optimize investment portfolios, detect fraud, and predict market trends.\n",
      "\n",
      "6. Healthcare: Reinforcement learning can be used to develop personalized treatment plans for patients based on their medical history and symptoms.\n",
      "\n",
      "7. Advertising: Reinforcement learning can be used to optimize ad placement and targeting to maximize revenue for advertisers.\n",
      "\n",
      "8. Supply chain management: Reinforcement learning can be used to optimize inventory management, demand forecasting, and logistics planning.\n",
      "\n",
      "9. Energy management: Reinforcement learning can be used to optimize energy consumption in buildings and power grids.\n",
      "\n",
      "10. Agriculture: Reinforcement learning can be used to optimize crop yield, reduce waste, and improve sustainability in agriculture.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Conversation_history = \"\"\"\n",
    "Human: {question}\n",
    "AI: {response}\n",
    "\"\"\".format(question=QUESTION, response=response)\n",
    "\n",
    "print (Conversation_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d99e34ad-5539-44dd-b080-3ad05efd2f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1. Use cases for reinforcement learning include robotics, game playing, autonomous vehicles, natural language processing, finance, healthcare, advertising, supply chain management, energy management, and agriculture.\n",
       "2. Reinforcement learning can be used to train machines to perform complex tasks and make real-time decisions.\n",
       "3. It has applications in various industries, including finance, healthcare, and agriculture.\n",
       "4. Reinforcement learning can optimize processes and improve efficiency, sustainability, and profitability."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printmd(chain.run({\"history\":Conversation_history, \"question\": FOLLOW_UP_QUESTION}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045e5af6-55d6-4353-b3f6-3275c95db00a",
   "metadata": {},
   "source": [
    "**Bingo!**, so we now know how to create a chatbot using LLMs, we just need to keep the state/history of the conversation and pass it as context every time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafd1694-0077-4aa8-bd01-e9f763ce36a3",
   "metadata": {},
   "source": [
    "## Now that we understand the concept of memory via adding history as a context, let's go back to our GPT Smart Search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba257e86-fd90-4a51-a72d-27000913e8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since Memory adds tokens to the prompt, we would need a better model that allows more space on the prompt\n",
    "#MODEL = \"gpt-35-turbo-16k\"\n",
    "MODEL = \"gpt-35-turbo\"\n",
    "COMPLETION_TOKENS = 1000\n",
    "llm = AzureChatOpenAI(deployment_name=MODEL, temperature=0.5, max_tokens=COMPLETION_TOKENS)\n",
    "embedder = OpenAIEmbeddings(deployment=\"text-embedding-ada-002\", chunk_size=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef9f459b-e8b8-40b9-a94d-80c079968594",
   "metadata": {},
   "outputs": [],
   "source": [
    "index1_name = \"cogsrch-index-files\"\n",
    "index2_name = \"cogsrch-index-csv\"\n",
    "index3_name = \"cogsrch-index-books-vector\"\n",
    "text_indexes = [index1_name, index2_name]\n",
    "vector_indexes = [index+\"-vector\" for index in text_indexes] + [index3_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b01852c2-6192-496c-adff-4270f9380469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results: 5\n",
      "CPU times: total: 3min 49s\n",
      "Wall time: 4min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Search in text-based indexes first and update vector indexes\n",
    "k=10 # Top k results per each text-based index\n",
    "ordered_results = get_search_results(QUESTION, text_indexes, k=k, reranker_threshold=1, vector_search=False)\n",
    "update_vector_indexes(ordered_search_results=ordered_results, embedder=embedder)\n",
    "\n",
    "# Search in all vector-based indexes available\n",
    "similarity_k = 5 # top results from multi-vector-index similarity search\n",
    "ordered_results = get_search_results(QUESTION, vector_indexes, k=k, vector_search=True,\n",
    "                                        similarity_k=similarity_k,\n",
    "                                        query_vector = embedder.embed_query(QUESTION))\n",
    "print(\"Number of results:\",len(ordered_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca500dd8-148c-4d8a-b58b-2df4c957459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the below line if you want to inspect the ordered results\n",
    "# ordered_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b2a3595-c3b7-4376-b9c5-0db7a42b3ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 5\n"
     ]
    }
   ],
   "source": [
    "top_docs = []\n",
    "for key,value in ordered_results.items():\n",
    "    location = value[\"location\"] if value[\"location\"] is not None else \"\"\n",
    "    top_docs.append(Document(page_content=value[\"chunk\"], metadata={\"source\": location+os.environ['BLOB_SAS_TOKEN']}))\n",
    "        \n",
    "print(\"Number of chunks:\",len(top_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c26d7540-feb8-4581-849e-003f4bf2a601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System prompt token count: 2464\n",
      "Max Completion Token count: 1000\n",
      "Combined docs (context) token count: 2322\n",
      "--------\n",
      "Requested token count: 5786\n",
      "Token limit for gpt-35-turbo : 4096\n",
      "Chain Type selected: map_reduce\n"
     ]
    }
   ],
   "source": [
    "# Calculate number of tokens of our docs\n",
    "if(len(top_docs)>0):\n",
    "    tokens_limit = model_tokens_limit(MODEL) # this is a custom function we created in common/utils.py\n",
    "    prompt_tokens = num_tokens_from_string(COMBINE_CHAT_PROMPT_TEMPLATE) # this is a custom function we created in common/utils.py\n",
    "    context_tokens = num_tokens_from_docs(top_docs) # this is a custom function we created in common/utils.py\n",
    "    \n",
    "    requested_tokens = prompt_tokens + context_tokens + COMPLETION_TOKENS\n",
    "    \n",
    "    chain_type = \"map_reduce\" if requested_tokens > 0.9 * tokens_limit else \"stuff\"  \n",
    "    \n",
    "    print(\"System prompt token count:\",prompt_tokens)\n",
    "    print(\"Max Completion Token count:\", COMPLETION_TOKENS)\n",
    "    print(\"Combined docs (context) token count:\",context_tokens)\n",
    "    print(\"--------\")\n",
    "    print(\"Requested token count:\",requested_tokens)\n",
    "    print(\"Token limit for\", MODEL, \":\", tokens_limit)\n",
    "    print(\"Chain Type selected:\", chain_type)\n",
    "        \n",
    "else:\n",
    "    print(\"NO RESULTS FROM AZURE SEARCH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ce6efa9-2b8f-4810-904d-5986b4ae0372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The provided portion of the documents does not contain information about use cases for reinforcement learning."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.36 s\n",
      "Wall time: 16.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Get the answer\n",
    "response = get_answer(llm=llm, docs=top_docs, query=QUESTION, language=\"English\", chain_type=chain_type)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27501f1b-7db0-4ee3-9cb1-e609254ffa3d",
   "metadata": {},
   "source": [
    "And if we ask the follow up question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5cf5b323-3b9c-479b-8502-acfc4f7915dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "There is no relevant information in the given portions of the documents to answer the question about the main points of a conversation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = get_answer(llm=llm, docs=top_docs,  query=FOLLOW_UP_QUESTION, language=\"English\", chain_type=chain_type)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035fa6e6-226c-400f-a504-30255385f43b",
   "metadata": {},
   "source": [
    "You might get a different response from above, but it doesn't matter what response you get, it will be based on the context given, not on previous answers.\n",
    "\n",
    "Until now we just have the same as the prior Notebook 03: results from Azure Search enhanced by OpenAI model, with no memory\n",
    "\n",
    "**Now let's add memory to it:**\n",
    "\n",
    "Reference: https://python.langchain.com/docs/modules/memory/how_to/adding_memory_chain_multiple_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d98b876e-d264-48ae-b5ed-9801d6a9152b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A single document was longer than the context length, we cannot handle this.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# memory object, which is neccessary to track the inputs/outputs and hold a conversation.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m memory \u001b[39m=\u001b[39m ConversationBufferMemory(memory_key\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mchat_history\u001b[39m\u001b[39m\"\u001b[39m,input_key\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m response \u001b[39m=\u001b[39m get_answer(llm\u001b[39m=\u001b[39;49mllm, docs\u001b[39m=\u001b[39;49mtop_docs, query\u001b[39m=\u001b[39;49mQUESTION, language\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEnglish\u001b[39;49m\u001b[39m\"\u001b[39;49m, chain_type\u001b[39m=\u001b[39;49mchain_type, \n\u001b[0;32m      5\u001b[0m                         memory\u001b[39m=\u001b[39;49mmemory)\n\u001b[0;32m      6\u001b[0m printmd(response[\u001b[39m'\u001b[39m\u001b[39moutput_text\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\code\\Azure-Cognitive-Search-Azure-OpenAI-Accelerator\\common\\utils.py:438\u001b[0m, in \u001b[0;36mget_answer\u001b[1;34m(llm, docs, query, language, chain_type, memory, callback_manager)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    436\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mError: chain_type\u001b[39m\u001b[39m\"\u001b[39m, chain_type, \u001b[39m\"\u001b[39m\u001b[39mnot supported\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 438\u001b[0m answer \u001b[39m=\u001b[39m chain( {\u001b[39m\"\u001b[39;49m\u001b[39minput_documents\u001b[39;49m\u001b[39m\"\u001b[39;49m: docs, \u001b[39m\"\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m\"\u001b[39;49m: query, \u001b[39m\"\u001b[39;49m\u001b[39mlanguage\u001b[39;49m\u001b[39m\"\u001b[39;49m: language}, return_only_outputs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    440\u001b[0m \u001b[39mreturn\u001b[39;00m answer\n",
      "File \u001b[1;32mc:\\Users\\gdardia\\miniconda3\\lib\\site-packages\\langchain\\chains\\base.py:243\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    242\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 243\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    244\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    245\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[0;32m    246\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    247\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\gdardia\\miniconda3\\lib\\site-packages\\langchain\\chains\\base.py:237\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[0;32m    231\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    232\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m    233\u001b[0m     inputs,\n\u001b[0;32m    234\u001b[0m )\n\u001b[0;32m    235\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    236\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 237\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    238\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    239\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[0;32m    240\u001b[0m     )\n\u001b[0;32m    241\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    242\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\gdardia\\miniconda3\\lib\\site-packages\\langchain\\chains\\combine_documents\\base.py:106\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[0;32m    105\u001b[0m other_keys \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key}\n\u001b[1;32m--> 106\u001b[0m output, extra_return_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcombine_docs(\n\u001b[0;32m    107\u001b[0m     docs, callbacks\u001b[39m=\u001b[39m_run_manager\u001b[39m.\u001b[39mget_child(), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mother_keys\n\u001b[0;32m    108\u001b[0m )\n\u001b[0;32m    109\u001b[0m extra_return_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key] \u001b[39m=\u001b[39m output\n\u001b[0;32m    110\u001b[0m \u001b[39mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[1;32mc:\\Users\\gdardia\\miniconda3\\lib\\site-packages\\langchain\\chains\\combine_documents\\map_reduce.py:221\u001b[0m, in \u001b[0;36mMapReduceDocumentsChain.combine_docs\u001b[1;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    215\u001b[0m question_result_key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_chain\u001b[39m.\u001b[39moutput_key\n\u001b[0;32m    216\u001b[0m result_docs \u001b[39m=\u001b[39m [\n\u001b[0;32m    217\u001b[0m     Document(page_content\u001b[39m=\u001b[39mr[question_result_key], metadata\u001b[39m=\u001b[39mdocs[i]\u001b[39m.\u001b[39mmetadata)\n\u001b[0;32m    218\u001b[0m     \u001b[39m# This uses metadata from the docs, and the textual results from `results`\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[39mfor\u001b[39;00m i, r \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(map_results)\n\u001b[0;32m    220\u001b[0m ]\n\u001b[1;32m--> 221\u001b[0m result, extra_return_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreduce_documents_chain\u001b[39m.\u001b[39mcombine_docs(\n\u001b[0;32m    222\u001b[0m     result_docs, token_max\u001b[39m=\u001b[39mtoken_max, callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    223\u001b[0m )\n\u001b[0;32m    224\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_intermediate_steps:\n\u001b[0;32m    225\u001b[0m     intermediate_steps \u001b[39m=\u001b[39m [r[question_result_key] \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m map_results]\n",
      "File \u001b[1;32mc:\\Users\\gdardia\\miniconda3\\lib\\site-packages\\langchain\\chains\\combine_documents\\reduce.py:195\u001b[0m, in \u001b[0;36mReduceDocumentsChain.combine_docs\u001b[1;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcombine_docs\u001b[39m(\n\u001b[0;32m    174\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    175\u001b[0m     docs: List[Document],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    179\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mstr\u001b[39m, \u001b[39mdict\u001b[39m]:\n\u001b[0;32m    180\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Combine multiple documents recursively.\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \n\u001b[0;32m    182\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[39m        element returned is a dictionary of other keys to return.\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 195\u001b[0m     result_docs, extra_return_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_collapse(\n\u001b[0;32m    196\u001b[0m         docs, token_max\u001b[39m=\u001b[39mtoken_max, callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    197\u001b[0m     )\n\u001b[0;32m    198\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcombine_documents_chain\u001b[39m.\u001b[39mcombine_docs(\n\u001b[0;32m    199\u001b[0m         docs\u001b[39m=\u001b[39mresult_docs, callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    200\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\gdardia\\miniconda3\\lib\\site-packages\\langchain\\chains\\combine_documents\\reduce.py:249\u001b[0m, in \u001b[0;36mReduceDocumentsChain._collapse\u001b[1;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    247\u001b[0m _token_max \u001b[39m=\u001b[39m token_max \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_max\n\u001b[0;32m    248\u001b[0m \u001b[39mwhile\u001b[39;00m num_tokens \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m num_tokens \u001b[39m>\u001b[39m _token_max:\n\u001b[1;32m--> 249\u001b[0m     new_result_doc_list \u001b[39m=\u001b[39m _split_list_of_docs(\n\u001b[0;32m    250\u001b[0m         result_docs, length_func, _token_max, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    251\u001b[0m     )\n\u001b[0;32m    252\u001b[0m     result_docs \u001b[39m=\u001b[39m []\n\u001b[0;32m    253\u001b[0m     \u001b[39mfor\u001b[39;00m docs \u001b[39min\u001b[39;00m new_result_doc_list:\n",
      "File \u001b[1;32mc:\\Users\\gdardia\\miniconda3\\lib\\site-packages\\langchain\\chains\\combine_documents\\reduce.py:38\u001b[0m, in \u001b[0;36m_split_list_of_docs\u001b[1;34m(docs, length_func, token_max, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mif\u001b[39;00m _num_tokens \u001b[39m>\u001b[39m token_max:\n\u001b[0;32m     37\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(_sub_result_docs) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 38\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     39\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mA single document was longer than the context length,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     40\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m we cannot handle this.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     41\u001b[0m         )\n\u001b[0;32m     42\u001b[0m     new_result_doc_list\u001b[39m.\u001b[39mappend(_sub_result_docs[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m     43\u001b[0m     _sub_result_docs \u001b[39m=\u001b[39m _sub_result_docs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:]\n",
      "\u001b[1;31mValueError\u001b[0m: A single document was longer than the context length, we cannot handle this."
     ]
    }
   ],
   "source": [
    "# memory object, which is neccessary to track the inputs/outputs and hold a conversation.\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",input_key=\"question\")\n",
    "\n",
    "response = get_answer(llm=llm, docs=top_docs, query=QUESTION, language=\"English\", chain_type=chain_type, \n",
    "                        memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf28927b-d9ee-4412-bb07-13e055e832a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are the main points of our conversation:\n",
       "\n",
       "1. Reinforcement learning has various use cases across different domains.\n",
       "2. Some examples of use cases for reinforcement learning include epidemic prevention strategies, sparse reward tasks, personalized recommendation systems, automatic feature engineering, and job scheduling in data centers.\n",
       "\n",
       "Would you like more information about any of these use cases?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we add a follow up question:\n",
    "response = get_answer(llm=llm, docs=top_docs, query=FOLLOW_UP_QUESTION, language=\"English\", chain_type=chain_type, \n",
    "                      memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3830b0b8-0ca2-4d0a-9747-f6273368002b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The main points of our conversation are as follows:\n",
       "\n",
       "1. Reinforcement learning has various use cases across different domains.\n",
       "2. Some examples of use cases for reinforcement learning include:\n",
       "   - Epidemic prevention strategies, where deep reinforcement learning is used to learn prevention strategies for infectious diseases<sup><a href=\"https://arxiv.org/pdf/2003.13676v1.pdf?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[1]</a></sup>.\n",
       "   - Sparse reward tasks, where self-imitation learning and exploration bonuses can be used to address the challenge of sparse rewards<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206262/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[2]</a></sup>.\n",
       "   - Personalized recommendation systems, where reinforcement learning can be applied to recommend personalized song sequences<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206183/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[3]</a></sup>.\n",
       "   - Automatic feature engineering, where reinforcement learning can be used to automate the process of feature engineering<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206177/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[4]</a></sup>.\n",
       "   - Job scheduling in data centers, where reinforcement learning can be applied to optimize job scheduling and resource allocation<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206316/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[5]</a></sup>.\n",
       "\n",
       "Please let me know if you would like more information about any of these use cases."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Another follow up query\n",
    "response = get_answer(llm=llm, docs=top_docs, query=\"Thank you\", language=\"English\", chain_type=chain_type,  \n",
    "                      memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111e732b-3c8c-4df3-8fcb-c3d01e7bec74",
   "metadata": {},
   "source": [
    "You might get a different answer on the above cell, and it is ok, this bot is not yet well configured to answer any question that is not related to its knowledge base, including salutations.\n",
    "\n",
    "Let's check our memory to see that it's keeping the conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1279692c-7eb0-4300-8a66-c7025f02c318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Tell me some use cases for reinforcement learning?\\nAI: Reinforcement learning has various use cases across different domains. Here are some examples:\\n\\n1. **Epidemic prevention strategies**: Reinforcement learning can be used to automatically learn prevention strategies for infectious diseases. For example, a study used deep reinforcement learning to learn mitigation policies in complex epidemiological models with a large state space<sup><a href=\"https://arxiv.org/pdf/2003.13676v1.pdf?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[1]</a></sup>.\\n\\n2. **Sparse reward tasks**: Reinforcement learning can be challenging when dealing with tasks that have sparse rewards. Self-imitation learning and exploration bonuses are two approaches that can address this challenge. A recent framework called Explore-then-Exploit (EE) interleaves self-imitation learning with an exploration bonus to enhance both exploitation and exploration in learning tasks<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206262/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[2]</a></sup>.\\n\\n3. **Personalized recommendation systems**: Reinforcement learning can be applied to personalized recommendation systems. For example, a personalized hybrid recommendation algorithm for music based on reinforcement learning was proposed. It uses techniques like weighted matrix factorization and convolutional neural networks to learn and extract song feature vectors, and it continuously updates the model based on the preferences of listeners for songs and song transitions<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206183/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[3]</a></sup>.\\n\\n4. **Automatic feature engineering**: Feature engineering is a crucial task in machine learning projects. Reinforcement learning can be used to automate feature engineering. For example, a framework called Cross-data Automatic Feature Engineering Machine (CAFEM) formalizes the feature engineering problem as an optimization problem over a Feature Transformation Graph (FTG). It includes a feature engineering learner that learns fine-grained strategies on a single dataset and a cross-data component that speeds up feature engineering learning on unseen datasets<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206177/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[4]</a></sup>.\\n\\n5. **Job scheduling in data centers**: Reinforcement learning can be applied to job scheduling in data centers. For example, an Advantage Actor-Critic (A2C) deep reinforcement learning-based approach called A2cScheduler was proposed for job scheduling. It consists of two agents, an actor and a critic, that work together to learn the scheduling policy and reduce estimation errors. The approach showed competitive performance using both simulated workloads and real data from an academic data center<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206316/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X\\nHuman: Give me the main points of our conversation\\nAI: Here are the main points of our conversation:\\n\\n1. Reinforcement learning has various use cases across different domains.\\n2. Some examples of use cases for reinforcement learning include epidemic prevention strategies, sparse reward tasks, personalized recommendation systems, automatic feature engineering, and job scheduling in data centers.\\n\\nWould you like more information about any of these use cases?\\nHuman: Thank you\\nAI: The main points of our conversation are as follows:\\n\\n1. Reinforcement learning has various use cases across different domains.\\n2. Some examples of use cases for reinforcement learning include:\\n   - Epidemic prevention strategies, where deep reinforcement learning is used to learn prevention strategies for infectious diseases<sup><a href=\"https://arxiv.org/pdf/2003.13676v1.pdf?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[1]</a></sup>.\\n   - Sparse reward tasks, where self-imitation learning and exploration bonuses can be used to address the challenge of sparse rewards<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206262/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[2]</a></sup>.\\n   - Personalized recommendation systems, where reinforcement learning can be applied to recommend personalized song sequences<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206183/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[3]</a></sup>.\\n   - Automatic feature engineering, where reinforcement learning can be used to automate the process of feature engineering<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206177/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[4]</a></sup>.\\n   - Job scheduling in data centers, where reinforcement learning can be applied to optimize job scheduling and resource allocation<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206316/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\" target=\"_blank\">[5]</a></sup>.\\n\\nPlease let me know if you would like more information about any of these use cases.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87405173",
   "metadata": {},
   "source": [
    "## Using CosmosDB as persistent memory\n",
    "\n",
    "In previous cell we have added local RAM memory to our chatbot. However, it is not persistent, it gets deleted once the app user's session is terminated. It is necessary then to use a Database for persistent storage of each of the bot user conversations, not only for Analytics and Auditing, but also if we wisg to provide recommendations. \n",
    "\n",
    "Here we will store the conversation history into CosmosDB for future auditing purpose.\n",
    "We will use a class in LangChain use CosmosDBChatMessageHistory, see [HERE](https://python.langchain.com/en/latest/_modules/langchain/memory/chat_message_histories/cosmos_db.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7131daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CosmosDB instance from langchain cosmos class.\n",
    "cosmos = CosmosDBChatMessageHistory(\n",
    "    cosmos_endpoint=os.environ['AZURE_COSMOSDB_ENDPOINT'],\n",
    "    cosmos_database=os.environ['AZURE_COSMOSDB_NAME'],\n",
    "    cosmos_container=os.environ['AZURE_COSMOSDB_CONTAINER_NAME'],\n",
    "    connection_string=os.environ['AZURE_COMOSDB_CONNECTION_STRING'],\n",
    "    session_id=\"Agent-Test-Session\" + str(random.randint(1, 1000)),\n",
    "    user_id=\"Agent-Test-User\" + str(random.randint(1, 1000))\n",
    "    )\n",
    "\n",
    "# prepare the cosmosdb instance\n",
    "cosmos.prepare_cosmos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d87cc7c6-5ef1-4492-b133-9f63a392e223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or Memory Object\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",input_key=\"question\",chat_memory=cosmos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27ceb47a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A single document was longer than the context length, we cannot handle this.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Testing using our Question\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m response \u001b[39m=\u001b[39m get_answer(llm\u001b[39m=\u001b[39;49mllm, docs\u001b[39m=\u001b[39;49mtop_docs, query\u001b[39m=\u001b[39;49mQUESTION, language\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mEnglish\u001b[39;49m\u001b[39m\"\u001b[39;49m, chain_type\u001b[39m=\u001b[39;49mchain_type, \n\u001b[0;32m      3\u001b[0m                         memory\u001b[39m=\u001b[39;49mmemory)\n\u001b[0;32m      4\u001b[0m printmd(response[\u001b[39m'\u001b[39m\u001b[39moutput_text\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\code\\Azure-Cognitive-Search-Azure-OpenAI-Accelerator\\common\\utils.py:438\u001b[0m, in \u001b[0;36mget_answer\u001b[1;34m(llm, docs, query, language, chain_type, memory, callback_manager)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    436\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mError: chain_type\u001b[39m\u001b[39m\"\u001b[39m, chain_type, \u001b[39m\"\u001b[39m\u001b[39mnot supported\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 438\u001b[0m answer \u001b[39m=\u001b[39m chain( {\u001b[39m\"\u001b[39;49m\u001b[39minput_documents\u001b[39;49m\u001b[39m\"\u001b[39;49m: docs, \u001b[39m\"\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m\"\u001b[39;49m: query, \u001b[39m\"\u001b[39;49m\u001b[39mlanguage\u001b[39;49m\u001b[39m\"\u001b[39;49m: language}, return_only_outputs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    440\u001b[0m \u001b[39mreturn\u001b[39;00m answer\n",
      "File \u001b[1;32mc:\\Users\\gdardia\\miniconda3\\lib\\site-packages\\langchain\\chains\\base.py:243\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    242\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 243\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    244\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    245\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[0;32m    246\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    247\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\gdardia\\miniconda3\\lib\\site-packages\\langchain\\chains\\base.py:237\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[0;32m    231\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    232\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m    233\u001b[0m     inputs,\n\u001b[0;32m    234\u001b[0m )\n\u001b[0;32m    235\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    236\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 237\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[0;32m    238\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    239\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[0;32m    240\u001b[0m     )\n\u001b[0;32m    241\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    242\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\gdardia\\miniconda3\\lib\\site-packages\\langchain\\chains\\combine_documents\\base.py:106\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[0;32m    105\u001b[0m other_keys \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key}\n\u001b[1;32m--> 106\u001b[0m output, extra_return_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcombine_docs(\n\u001b[0;32m    107\u001b[0m     docs, callbacks\u001b[39m=\u001b[39m_run_manager\u001b[39m.\u001b[39mget_child(), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mother_keys\n\u001b[0;32m    108\u001b[0m )\n\u001b[0;32m    109\u001b[0m extra_return_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key] \u001b[39m=\u001b[39m output\n\u001b[0;32m    110\u001b[0m \u001b[39mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[1;32mc:\\Users\\gdardia\\miniconda3\\lib\\site-packages\\langchain\\chains\\combine_documents\\map_reduce.py:221\u001b[0m, in \u001b[0;36mMapReduceDocumentsChain.combine_docs\u001b[1;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    215\u001b[0m question_result_key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_chain\u001b[39m.\u001b[39moutput_key\n\u001b[0;32m    216\u001b[0m result_docs \u001b[39m=\u001b[39m [\n\u001b[0;32m    217\u001b[0m     Document(page_content\u001b[39m=\u001b[39mr[question_result_key], metadata\u001b[39m=\u001b[39mdocs[i]\u001b[39m.\u001b[39mmetadata)\n\u001b[0;32m    218\u001b[0m     \u001b[39m# This uses metadata from the docs, and the textual results from `results`\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[39mfor\u001b[39;00m i, r \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(map_results)\n\u001b[0;32m    220\u001b[0m ]\n\u001b[1;32m--> 221\u001b[0m result, extra_return_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreduce_documents_chain\u001b[39m.\u001b[39mcombine_docs(\n\u001b[0;32m    222\u001b[0m     result_docs, token_max\u001b[39m=\u001b[39mtoken_max, callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    223\u001b[0m )\n\u001b[0;32m    224\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_intermediate_steps:\n\u001b[0;32m    225\u001b[0m     intermediate_steps \u001b[39m=\u001b[39m [r[question_result_key] \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m map_results]\n",
      "File \u001b[1;32mc:\\Users\\gdardia\\miniconda3\\lib\\site-packages\\langchain\\chains\\combine_documents\\reduce.py:195\u001b[0m, in \u001b[0;36mReduceDocumentsChain.combine_docs\u001b[1;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcombine_docs\u001b[39m(\n\u001b[0;32m    174\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    175\u001b[0m     docs: List[Document],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    179\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mstr\u001b[39m, \u001b[39mdict\u001b[39m]:\n\u001b[0;32m    180\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Combine multiple documents recursively.\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \n\u001b[0;32m    182\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[39m        element returned is a dictionary of other keys to return.\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 195\u001b[0m     result_docs, extra_return_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_collapse(\n\u001b[0;32m    196\u001b[0m         docs, token_max\u001b[39m=\u001b[39mtoken_max, callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    197\u001b[0m     )\n\u001b[0;32m    198\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcombine_documents_chain\u001b[39m.\u001b[39mcombine_docs(\n\u001b[0;32m    199\u001b[0m         docs\u001b[39m=\u001b[39mresult_docs, callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    200\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\gdardia\\miniconda3\\lib\\site-packages\\langchain\\chains\\combine_documents\\reduce.py:249\u001b[0m, in \u001b[0;36mReduceDocumentsChain._collapse\u001b[1;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    247\u001b[0m _token_max \u001b[39m=\u001b[39m token_max \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_max\n\u001b[0;32m    248\u001b[0m \u001b[39mwhile\u001b[39;00m num_tokens \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m num_tokens \u001b[39m>\u001b[39m _token_max:\n\u001b[1;32m--> 249\u001b[0m     new_result_doc_list \u001b[39m=\u001b[39m _split_list_of_docs(\n\u001b[0;32m    250\u001b[0m         result_docs, length_func, _token_max, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    251\u001b[0m     )\n\u001b[0;32m    252\u001b[0m     result_docs \u001b[39m=\u001b[39m []\n\u001b[0;32m    253\u001b[0m     \u001b[39mfor\u001b[39;00m docs \u001b[39min\u001b[39;00m new_result_doc_list:\n",
      "File \u001b[1;32mc:\\Users\\gdardia\\miniconda3\\lib\\site-packages\\langchain\\chains\\combine_documents\\reduce.py:38\u001b[0m, in \u001b[0;36m_split_list_of_docs\u001b[1;34m(docs, length_func, token_max, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mif\u001b[39;00m _num_tokens \u001b[39m>\u001b[39m token_max:\n\u001b[0;32m     37\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(_sub_result_docs) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 38\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     39\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mA single document was longer than the context length,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     40\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m we cannot handle this.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     41\u001b[0m         )\n\u001b[0;32m     42\u001b[0m     new_result_doc_list\u001b[39m.\u001b[39mappend(_sub_result_docs[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m     43\u001b[0m     _sub_result_docs \u001b[39m=\u001b[39m _sub_result_docs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:]\n",
      "\u001b[1;31mValueError\u001b[0m: A single document was longer than the context length, we cannot handle this."
     ]
    }
   ],
   "source": [
    "# Testing using our Question\n",
    "response = get_answer(llm=llm, docs=top_docs, query=QUESTION, language=\"English\", chain_type=chain_type, \n",
    "                        memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a5ff826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are the main points of our conversation:\n",
       "\n",
       "1. Reinforcement learning has various use cases in different domains.\n",
       "2. One use case is epidemic prevention, where deep reinforcement learning is used to learn prevention strategies for infectious diseases<sup><a href=\"https://arxiv.org/pdf/2003.13676v1.pdf?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\">[1]</a></sup>.\n",
       "3. Another use case is tackling tasks with sparse rewards, where self-imitation learning and exploration bonuses are used to enhance exploitation and exploration<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206262/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\">[2]</a></sup>.\n",
       "4. Reinforcement learning can be used to improve personalized recommendation systems, such as a hybrid recommendation algorithm for music based on reinforcement learning<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206183/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\">[3]</a></sup>.\n",
       "5. Reinforcement learning can automate feature engineering tasks, such as a framework called CAFEM that learns fine-grained feature engineering strategies using reinforcement learning<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206177/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\">[4]</a></sup>.\n",
       "6. Reinforcement learning can be used for efficient job scheduling in data centers, such as the A2cScheduler approach based on Advantage Actor-Critic (A2C) deep reinforcement learning<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206316/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\">[5]</a></sup>.\n",
       "\n",
       "These points summarize the main use cases of reinforcement learning that we discussed. Is there anything else I can help you with?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we add a follow up question:\n",
    "response = get_answer(llm=llm, docs=top_docs, query=FOLLOW_UP_QUESTION, language=\"English\", chain_type=chain_type, \n",
    "                      memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be1620fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on our conversation, here are the main points:\n",
       "\n",
       "1. Reinforcement learning has various use cases in different domains.\n",
       "2. One use case is epidemic prevention, where deep reinforcement learning is used to learn prevention strategies for infectious diseases<sup><a href=\"https://arxiv.org/pdf/2003.13676v1.pdf?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\">[1]</a></sup>.\n",
       "3. Another use case is tackling tasks with sparse rewards, where self-imitation learning and exploration bonuses are used to enhance exploitation and exploration<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206262/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\">[2]</a></sup>.\n",
       "4. Reinforcement learning can be used to improve personalized recommendation systems, such as a hybrid recommendation algorithm for music based on reinforcement learning<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206183/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\">[3]</a></sup>.\n",
       "5. Reinforcement learning can automate feature engineering tasks, such as a framework called CAFEM that learns fine-grained feature engineering strategies using reinforcement learning<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206177/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\">[4]</a></sup>.\n",
       "6. Reinforcement learning can be used for efficient job scheduling in data centers, such as the A2cScheduler approach based on Advantage Actor-Critic (A2C) deep reinforcement learning<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206316/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\">[5]</a></sup>.\n",
       "\n",
       "These points summarize the main use cases of reinforcement learning that we discussed. Let me know if there's anything else I can assist you with."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Another follow up query\n",
    "response = get_answer(llm=llm, docs=top_docs, query=\"Thank you\", language=\"English\", chain_type=chain_type,  \n",
    "                      memory=memory)\n",
    "printmd(response['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc5ac98",
   "metadata": {},
   "source": [
    "Let's check our Azure CosmosDB to see the whole conversation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1d7688a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Tell me some use cases for reinforcement learning?', additional_kwargs={}, example=False),\n",
       " AIMessage(content='Reinforcement learning has various use cases in different domains. Here are some examples:\\n\\n1. **Epidemic Prevention**: Reinforcement learning can be used to automatically learn prevention strategies for infectious diseases. For example, a study used deep reinforcement learning to learn mitigation policies in complex epidemiological models with a large state space<sup><a href=\"https://arxiv.org/pdf/2003.13676v1.pdf?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\">[1]</a></sup>.\\n\\n2. **Sparse Reward Tasks**: Reinforcement learning is used to tackle tasks with sparse rewards, where efficient exploitation and exploration are required. One approach is self-imitation learning, which encourages exploitation by imitating past good trajectories. Another approach is exploration bonuses, which enhance exploration by providing intrinsic rewards. A novel framework called Explore-then-Exploit (EE) has been introduced, which interleaves self-imitation learning with an exploration bonus to strengthen the effect of these two algorithms<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206262/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\">[2]</a></sup>.\\n\\n3. **Personalized Recommendation Systems**: Reinforcement learning can be used to improve personalized recommendation systems. For example, a personalized hybrid recommendation algorithm for music based on reinforcement learning was proposed. It recommends song sequences that match listeners\\' preferences better by simulating the interaction process and updating the model continuously based on their preferences<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206183/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\">[3]</a></sup>.\\n\\n4. **Feature Engineering**: Reinforcement learning can be used to automate feature engineering, which is a time-consuming and challenging task in machine learning projects. A framework called Cross-data Automatic Feature Engineering Machine (CAFEM) has been proposed, which formalizes the feature engineering problem as an optimization problem and learns fine-grained feature engineering strategies using reinforcement learning<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206177/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\">[4]</a></sup>.\\n\\n5. **Job Scheduling**: Reinforcement learning can be used for efficient job scheduling in data centers. An approach called A2cScheduler, based on Advantage Actor-Critic (A2C) deep reinforcement learning, has been proposed for job scheduling. It consists of two agents, the actor and the critic, which learn the scheduling policy and reduce estimation error, respectively<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206316/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\">[5]</a></sup>.\\n\\nThese are just a few examples of the use cases for reinforcement learning. It', additional_kwargs={}, example=False),\n",
       " HumanMessage(content='Give me the main points of our conversation', additional_kwargs={}, example=False),\n",
       " AIMessage(content='Here are the main points of our conversation:\\n\\n1. Reinforcement learning has various use cases in different domains.\\n2. One use case is epidemic prevention, where deep reinforcement learning is used to learn prevention strategies for infectious diseases<sup><a href=\"https://arxiv.org/pdf/2003.13676v1.pdf?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\">[1]</a></sup>.\\n3. Another use case is tackling tasks with sparse rewards, where self-imitation learning and exploration bonuses are used to enhance exploitation and exploration<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206262/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\">[2]</a></sup>.\\n4. Reinforcement learning can be used to improve personalized recommendation systems, such as a hybrid recommendation algorithm for music based on reinforcement learning<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206183/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\">[3]</a></sup>.\\n5. Reinforcement learning can automate feature engineering tasks, such as a framework called CAFEM that learns fine-grained feature engineering strategies using reinforcement learning<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206177/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\">[4]</a></sup>.\\n6. Reinforcement learning can be used for efficient job scheduling in data centers, such as the A2cScheduler approach based on Advantage Actor-Critic (A2C) deep reinforcement learning<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206316/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\">[5]</a></sup>.\\n\\nThese points summarize the main use cases of reinforcement learning that we discussed. Is there anything else I can help you with?', additional_kwargs={}, example=False),\n",
       " HumanMessage(content='Thank you', additional_kwargs={}, example=False),\n",
       " AIMessage(content='Based on our conversation, here are the main points:\\n\\n1. Reinforcement learning has various use cases in different domains.\\n2. One use case is epidemic prevention, where deep reinforcement learning is used to learn prevention strategies for infectious diseases<sup><a href=\"https://arxiv.org/pdf/2003.13676v1.pdf?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\">[1]</a></sup>.\\n3. Another use case is tackling tasks with sparse rewards, where self-imitation learning and exploration bonuses are used to enhance exploitation and exploration<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206262/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\">[2]</a></sup>.\\n4. Reinforcement learning can be used to improve personalized recommendation systems, such as a hybrid recommendation algorithm for music based on reinforcement learning<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206183/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\">[3]</a></sup>.\\n5. Reinforcement learning can automate feature engineering tasks, such as a framework called CAFEM that learns fine-grained feature engineering strategies using reinforcement learning<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206177/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\">[4]</a></sup>.\\n6. Reinforcement learning can be used for efficient job scheduling in data centers, such as the A2cScheduler approach based on Advantage Actor-Critic (A2C) deep reinforcement learning<sup><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7206316/?sv=2022-11-02&ss=bf&srt=sco&sp=rltfx&se=2024-10-02T01:02:07Z&st=2023-08-03T17:02:07Z&spr=https&sig=gLxStXFSY6X29OPpPDpBEhoQDdtJNDrMVExNYJ%2BhmBQ%3D\">[5]</a></sup>.\\n\\nThese points summarize the main use cases of reinforcement learning that we discussed. Let me know if there\\'s anything else I can assist you with.', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load message from cosmosdb\n",
    "cosmos.load_messages()\n",
    "cosmos.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e30694-ae2a-47bb-a5c7-db51ecdbba1e",
   "metadata": {},
   "source": [
    "![CosmosDB Memory](./images/cosmos-chathistory.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6789cada-23a3-451a-a91a-0906ceb0bd14",
   "metadata": {},
   "source": [
    "# Summary\n",
    "##### Adding memory to our application allows the user to have a conversation, however this feature is not something that comes with the LLM, but instead, memory is something that we must provide to the LLM in form of context of the question.\n",
    "\n",
    "We added persitent memory using CosmosDB.\n",
    "\n",
    "We also can notice that the current chain that we are using is smart, but not that much. Although we have given memory to it, it searches for similar docs everytime, regardless of the input and it struggles to respond to prompts like: Hello, Thank you, Bye, What's your name, What's the weather and any other task that is not search in the knowledge base.\n",
    "\n",
    "\n",
    "## <u>Important Note</u>:<br>\n",
    "As we proceed, while all the code will remain compatible with GPT-3.5 models, we highly recommend transitioning to GPT-4. Here's why:\n",
    "\n",
    "**GPT-3.5-Turbo** can be likened to a 7-year-old child. You can provide it with concise instructions, but it frequently struggles to follow them accurately. Additionally, its limited memory can make sustained conversations challenging.\n",
    "\n",
    "**GPT-3.5-Turbo-16k** resembles the same 7-year-old, but with an increased attention span for longer instructions. However, it still faces difficulties accurately executing them about half the time.\n",
    "\n",
    "**GPT-4** exhibits the capabilities of a 10-12-year-old child. It possesses enhanced reasoning skills and more consistently adheres to instructions. While its memory retention for instructions is moderate, it excels at following them.\n",
    "\n",
    "**GPT-4-32k** is akin to the 10-12-year-old child with an extended memory. It comprehends lengthy sets of instructions and engages in meaningful conversations. Thanks to its robust memory, it offers detailed responses.\n",
    "\n",
    "Understanding this analogy above will become clearer as you complete the final notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c629ebf4-aced-45b7-a6a2-315810d37d48",
   "metadata": {},
   "source": [
    "# NEXT\n",
    "We know now how to do a Smart Search Engine that can power a chatbot!! great!\n",
    "\n",
    "But, does this solve all the possible scenarios that a virtual assistant will require?  **What about if the answer to the Smart Search Engine is not related to text, but instead requires to look into tabular data?** The next notebook explains and solves the tabular problem and the concept of Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4d9da4-3918-4da6-b235-a3320f0dcb12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
